{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib2 in /opt/homebrew/lib/python3.11/site-packages (2.3.7.post1)\n",
      "Requirement already satisfied: six in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from pathlib2) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting edgar\n",
      "  Downloading edgar-5.6.3-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from edgar) (2.31.0)\n",
      "Requirement already satisfied: lxml in /opt/homebrew/lib/python3.11/site-packages (from edgar) (4.9.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from edgar) (4.66.5)\n",
      "Collecting rapidfuzz (from edgar)\n",
      "  Downloading rapidfuzz-3.10.0-cp311-cp311-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from requests->edgar) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from requests->edgar) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from requests->edgar) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from requests->edgar) (2023.7.22)\n",
      "Installing collected packages: rapidfuzz, edgar\n",
      "Successfully installed edgar-5.6.3 rapidfuzz-3.10.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pathlib2\n",
    "!pip install edgar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edgar\n",
    "import os, time\n",
    "from pathlib2 import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import ProjectDirectory as directory\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate df with all companies and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = project_dir = Path('/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/')\n",
    "os.chdir(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(project_dir, 'sec-filings-index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filing_year = 2013   # uncomment to run, choose year to get all edgar filings from\n",
    "# edgar.download_index(os.getcwd(), filing_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all DFs \n",
    "table_list = []\n",
    "\n",
    "for i in os.listdir():\n",
    "    if i.endswith('.tsv'):\n",
    "        table_list.append(pd.read_csv(i, sep='|', header=None, encoding='latin-1', parse_dates=[3], dtype={0: int}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all dfs into a single df\n",
    "df = pd.DataFrame(columns=[0,1,2,3,4,5])   # downloaded file has 6 columns\n",
    "\n",
    "for i in range(len(table_list)):\n",
    "        df = pd.concat([df, table_list[i]], ignore_index=True, axis=0)\n",
    "\n",
    "df.columns= ['cik', 'company_name', 'filing_type', 'filing_date', 'url', 'url2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if dataframe correctly generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df tallies with individual files. Total rows = 6061186\n"
     ]
    }
   ],
   "source": [
    "count_list = []\n",
    "for i in range(len(table_list)):\n",
    "    count_list.append(len(table_list[i]))\n",
    "\n",
    "if df.shape[0] == sum(count_list):\n",
    "    print('df tallies with individual files. Total rows = {}'.format(df.shape[0]))\n",
    "else:\n",
    "    print('ERROR. df does not tally!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CIK df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cik_ticker_list.csv contains cik tickets of companies\n",
    "df_cik = pd.read_csv(os.path.join(project_dir, 'data', 'cik_ticker_list.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_name_search(df, company_name_list):\n",
    "    for company in company_name_list:\n",
    "        df_company = df[df['Name'].str.contains(company, case=False)]\n",
    "        print('*' * 50)\n",
    "        print('SEARCH TERM: {}'.format(company))\n",
    "        print('RESULTS:')\n",
    "        for i in df_company['Name'].tolist():\n",
    "            for j in df_company['CIK'].tolist():\n",
    "                print(i, j)\n",
    "        print('*' * 50)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik_from_company_name(df, company_name_list):\n",
    "    cik_list = []\n",
    "    for company in company_name_list:\n",
    "        cik_series = df[df['Name'].str.contains(company, case=False)]['CIK']\n",
    "        cik_list.append(cik_series.values[0])\n",
    "    return cik_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name_from_cik(df, cik_list):\n",
    "    company_list = []\n",
    "    for cik in cik_list:\n",
    "        company_series = df[df['CIK'] == cik]\n",
    "        company_list.append(company_series.values[0])\n",
    "    return company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_list = ['apple inc', 'tesla', 'netflix', \n",
    "                  'amazon com inc', 'microsoft', 'facebook', 'General Motors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "SEARCH TERM: apple inc\n",
      "RESULTS:\n",
      "Apple Inc 320193\n",
      "**************************************************\n",
      "**************************************************\n",
      "SEARCH TERM: tesla\n",
      "RESULTS:\n",
      "Tesla Motors Inc 1318605\n",
      "**************************************************\n",
      "**************************************************\n",
      "SEARCH TERM: netflix\n",
      "RESULTS:\n",
      "Netflix Inc 1065280\n",
      "**************************************************\n",
      "**************************************************\n",
      "SEARCH TERM: amazon com inc\n",
      "RESULTS:\n",
      "Amazon Com Inc 1018724\n",
      "**************************************************\n",
      "**************************************************\n",
      "SEARCH TERM: microsoft\n",
      "RESULTS:\n",
      "Microsoft Corp 789019\n",
      "**************************************************\n",
      "**************************************************\n",
      "SEARCH TERM: facebook\n",
      "RESULTS:\n",
      "Facebook Inc 1326801\n",
      "**************************************************\n",
      "**************************************************\n",
      "SEARCH TERM: General Motors\n",
      "RESULTS:\n",
      "General Motors Financial Company Inc 804269\n",
      "General Motors Financial Company Inc 1467858\n",
      "General Motors Co 804269\n",
      "General Motors Co 1467858\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "company_name_search(df_cik, companies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list = get_cik_from_company_name(df_cik, companies_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_filings(cik_num_list, from_date='2014-01-01'):\n",
    "    \"\"\"Function to filter the appropriate filings and download them in the folder\"\"\"\n",
    "    \n",
    "    project_dir = project_dir = Path('/Users/rishikumra/Downloads/sentiment-analysis-sec-master/')\n",
    "    os.chdir(project_dir)\n",
    "    \n",
    "    # filter df with company CIK,filing type (10-K and 10-Q) and date  \n",
    "    df_filtered = df [(df['cik'].isin(cik_num_list)) & \n",
    "                      ((df['filing_type']=='10-K') | (df['filing_type'] == '10-Q')) & \n",
    "                      (df['filing_date'] > from_date)]\n",
    "    \n",
    "    company_names = df_filtered['company_name'].unique().tolist()\n",
    "    \n",
    "    # check if folders for each company already exists    \n",
    "    sec_filings_dir = os.path.join(project_dir, 'sec-filings-downloaded')  # dir to download SEC filingsa\n",
    "    os.chdir(sec_filings_dir)\n",
    "\n",
    "    for company in company_names:\n",
    "        company_dir = os.path.join(sec_filings_dir, company)\n",
    "\n",
    "        if not os.path.exists(company_dir):\n",
    "            os.makedirs(company_dir)\n",
    "            print('\\n created dir: {}'.format(company))\n",
    "        else:\n",
    "            print('\\n{} directory exists'.format(company))\n",
    "            \n",
    "        os.chdir(company_dir)\n",
    "        \n",
    "        # create company specific df to iterate over    \n",
    "        df_filtered_co = df_filtered[df_filtered['company_name'] == company]  # get df with the company only\n",
    "        df_filtered_co['filing_date'] = df_filtered_co['filing_date'].astype(str)   # convert to 'object' to name file\n",
    "        \n",
    "        for i in range(len(df_filtered_co)):\n",
    "            url_prefix = 'https://www.sec.gov/Archives/'\n",
    "            row = df_filtered_co.iloc[i,:]\n",
    "            url = url_prefix + row['url']\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            \n",
    "            filing_name = row['filing_date'] + str('_') + row['filing_type']\n",
    "            if os.path.isfile(filing_name):\n",
    "                print('{} file already exists'.format(filing_name))\n",
    "            else:\n",
    "                print('Downloading: {}'.format(filing_name))\n",
    "                with open('{}'.format(filing_name), 'wb') as handle:\n",
    "                    for data in tqdm(response.iter_content()):\n",
    "                        handle.write(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ↓ Automated download of filings. If the filing exists in the directory, the download will skip and move on the the next filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AMAZON COM INC directory exists\n",
      "2017-02-10_10-K file already exists\n",
      "2016-01-29_10-K file already exists\n",
      "2017-07-28_10-Q file already exists\n",
      "2016-07-29_10-Q file already exists\n",
      "2016-04-29_10-Q file already exists\n",
      "2017-04-28_10-Q file already exists\n",
      "2017-10-27_10-Q file already exists\n",
      "2016-10-28_10-Q file already exists\n",
      "2019-02-01_10-K file already exists\n",
      "2018-02-02_10-K file already exists\n",
      "2018-07-27_10-Q file already exists\n",
      "2018-04-27_10-Q file already exists\n",
      "2018-10-26_10-Q file already exists\n",
      "2014-10-24_10-Q file already exists\n",
      "2015-10-23_10-Q file already exists\n",
      "2014-07-25_10-Q file already exists\n",
      "2015-07-24_10-Q file already exists\n",
      "2015-04-24_10-Q file already exists\n",
      "2014-04-25_10-Q file already exists\n",
      "2014-01-31_10-K file already exists\n",
      "2015-01-30_10-K file already exists\n",
      "\n",
      "NETFLIX INC directory exists\n",
      "2017-01-27_10-K file already exists\n",
      "2016-01-28_10-K file already exists\n",
      "2017-07-19_10-Q file already exists\n",
      "2016-07-19_10-Q file already exists\n",
      "2016-04-20_10-Q file already exists\n",
      "2017-04-20_10-Q file already exists\n",
      "2017-10-18_10-Q file already exists\n",
      "2016-10-20_10-Q file already exists\n",
      "2019-01-29_10-K file already exists\n",
      "2018-01-29_10-K file already exists\n",
      "2018-07-18_10-Q file already exists\n",
      "2018-04-18_10-Q file already exists\n",
      "2018-10-18_10-Q file already exists\n",
      "2014-10-20_10-Q file already exists\n",
      "2015-10-16_10-Q file already exists\n",
      "2014-07-22_10-Q file already exists\n",
      "2015-07-17_10-Q file already exists\n",
      "2015-04-17_10-Q file already exists\n",
      "2014-04-23_10-Q file already exists\n",
      "2014-02-03_10-K file already exists\n",
      "2015-01-29_10-K file already exists\n",
      "\n",
      "Tesla, Inc. directory exists\n",
      "2017-03-01_10-K file already exists\n",
      "2017-08-04_10-Q file already exists\n",
      "2017-05-10_10-Q file already exists\n",
      "2017-11-03_10-Q file already exists\n",
      "2019-02-19_10-K file already exists\n",
      "2018-02-23_10-K file already exists\n",
      "2018-08-06_10-Q file already exists\n",
      "2018-05-07_10-Q file already exists\n",
      "2018-11-02_10-Q file already exists\n",
      "\n",
      "Facebook Inc directory exists\n",
      "2017-02-03_10-K file already exists\n",
      "2016-01-28_10-K file already exists\n",
      "2017-07-27_10-Q file already exists\n",
      "2016-07-28_10-Q file already exists\n",
      "2016-04-28_10-Q file already exists\n",
      "2017-05-04_10-Q file already exists\n",
      "2017-11-02_10-Q file already exists\n",
      "2016-11-03_10-Q file already exists\n",
      "2019-01-31_10-K file already exists\n",
      "2018-02-01_10-K file already exists\n",
      "2018-07-26_10-Q file already exists\n",
      "2018-04-26_10-Q file already exists\n",
      "2018-10-31_10-Q file already exists\n",
      "2014-10-30_10-Q file already exists\n",
      "2015-11-05_10-Q file already exists\n",
      "2014-07-24_10-Q file already exists\n",
      "2015-07-31_10-Q file already exists\n",
      "2015-04-23_10-Q file already exists\n",
      "2014-04-25_10-Q file already exists\n",
      "2014-01-31_10-K file already exists\n",
      "2015-01-29_10-K file already exists\n",
      "\n",
      "APPLE INC directory exists\n",
      "2017-02-01_10-Q file already exists\n",
      "2016-01-27_10-Q file already exists\n",
      "2017-08-02_10-Q file already exists\n",
      "2016-07-27_10-Q file already exists\n",
      "2016-04-27_10-Q file already exists\n",
      "2017-05-03_10-Q file already exists\n",
      "2017-11-03_10-K file already exists\n",
      "2016-10-26_10-K file already exists\n",
      "2019-01-30_10-Q file already exists\n",
      "2018-02-02_10-Q file already exists\n",
      "2018-08-01_10-Q file already exists\n",
      "2018-05-02_10-Q file already exists\n",
      "2018-11-05_10-K file already exists\n",
      "2014-10-27_10-K file already exists\n",
      "2015-10-28_10-K file already exists\n",
      "2014-07-23_10-Q file already exists\n",
      "2015-07-22_10-Q file already exists\n",
      "2015-04-28_10-Q file already exists\n",
      "2014-04-24_10-Q file already exists\n",
      "2014-01-28_10-Q file already exists\n",
      "2015-01-28_10-Q file already exists\n",
      "\n",
      "MICROSOFT CORP directory exists\n",
      "2017-01-26_10-Q file already exists\n",
      "2016-01-28_10-Q file already exists\n",
      "2017-08-02_10-K file already exists\n",
      "2016-07-28_10-K file already exists\n",
      "2016-04-21_10-Q file already exists\n",
      "2017-04-27_10-Q file already exists\n",
      "2017-10-26_10-Q file already exists\n",
      "2016-10-20_10-Q file already exists\n",
      "2019-01-30_10-Q file already exists\n",
      "2018-01-31_10-Q file already exists\n",
      "2018-08-03_10-K file already exists\n",
      "2018-04-26_10-Q file already exists\n",
      "2018-10-24_10-Q file already exists\n",
      "2014-10-23_10-Q file already exists\n",
      "2015-10-22_10-Q file already exists\n",
      "2014-07-31_10-K file already exists\n",
      "2015-07-31_10-K file already exists\n",
      "2015-04-23_10-Q file already exists\n",
      "2014-04-24_10-Q file already exists\n",
      "2014-01-23_10-Q file already exists\n",
      "2015-01-26_10-Q file already exists\n",
      "\n",
      " created dir: General Motors Financial Company, Inc.\n",
      "Downloading: 2017-02-07_10-K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487it [00:00, 47462.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2016-02-03_10-K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1490it [00:00, 47407.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2017-07-25_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1478it [00:00, 64739.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2016-07-21_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1490it [00:00, 64099.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2016-04-21_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1478it [00:00, 43382.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2017-04-28_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1473it [00:00, 44315.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2017-10-24_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1490it [00:00, 63754.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2016-10-25_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1500it [00:00, 66628.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2019-02-06_10-K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1490it [00:00, 71841.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2018-02-06_10-K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487it [00:00, 52662.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2018-07-25_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1477it [00:00, 49771.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2018-04-26_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1473it [00:00, 58957.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2018-10-31_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487it [00:00, 33169.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2014-10-23_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1478it [00:00, 59340.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2015-10-21_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [00:00, 75035.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2014-07-24_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1491it [00:00, 65206.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2015-07-23_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1495it [00:00, 43536.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2015-04-23_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1478it [00:00, 47250.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2014-04-24_10-Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1491it [00:00, 37234.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2014-02-06_10-K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1490it [00:00, 44670.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2015-02-04_10-K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1504it [00:00, 56325.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESLA MOTORS INC directory exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-02-24_10-K file already exists\n",
      "2016-08-05_10-Q file already exists\n",
      "2016-05-10_10-Q file already exists\n",
      "2016-11-02_10-Q file already exists\n",
      "2014-11-07_10-Q file already exists\n",
      "2015-11-05_10-Q file already exists\n",
      "2014-08-08_10-Q file already exists\n",
      "2015-08-07_10-Q file already exists\n",
      "2015-05-11_10-Q file already exists\n",
      "2014-05-09_10-Q file already exists\n",
      "2014-02-26_10-K file already exists\n",
      "2015-02-26_10-K file already exists\n"
     ]
    }
   ],
   "source": [
    "download_filings(cik_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
