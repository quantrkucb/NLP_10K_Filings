{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.2 (main, Feb 16 2023, 02:51:42) [Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "pandas version: 2.2.3\n",
      "matplotlib version: 3.8.3\n",
      "NumPy version: 1.26.4\n",
      "SciPy version: 1.11.1\n",
      "IPython version: 8.14.0\n",
      "scikit-learn version: 1.4.1.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rishikumra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/homebrew/lib/python3.11/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/homebrew/lib/python3.11/site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/homebrew/lib/python3.11/site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (from wordcloud) (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rishikumra/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib2 import Path\n",
    "import re\n",
    "import shutil\n",
    "import ProjectDirectory as directory\n",
    "\n",
    "# preprocess filings\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# to vectorize filing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries for text\n",
    "\n",
    "# !pip install PyDrive\n",
    "# !pip install gensim\n",
    "# !pip install pyldavis\n",
    "# !python -m spacy download en\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import urllib.request as url\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import gzip\n",
    "# spacy.load('en_core_web_sm')\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import stopwords from LoughranMcDonald Master Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_master_dict_stopwords(stopwords_file_path = os.path.join(directory.get_project_dir(), 'master-dict', 'StopWords_Generic.txt')):\n",
    "#     os.chdir(stopwords_file_dir)\n",
    "#     stopwords = pd.read_csv('StopWords_Generic.txt', header=None)\n",
    "    stopwords = pd.read_csv(stopwords_file_path, header=None)[0].tolist()\n",
    "    stopwords = frozenset([word.lower() for word in stopwords])\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_filing(text, stopwords=True, stemming=False):\n",
    "    \n",
    "    # remove punctuations\n",
    "    punctuation_list = set(string.punctuation)\n",
    "    text = ''.join(word for word in text if word not in punctuation_list)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if stopwords:\n",
    "        stopwords = import_master_dict_stopwords()\n",
    "        tokens = [word for word in tokens if word not in stopwords]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "                \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_preprocess_filings(filings_list):\n",
    "    \"\"\"vectorizes and preprocesses filings for each company\"\"\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=preprocess_filing)\n",
    "    X = vectorizer.fit_transform(filings_list)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consine_similarity(a, b):\n",
    "    cos_sim = np.dot(a,b) / ( np.linalg.norm(a) * np.linalg.norm(b) )\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2345"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = pd.read_csv('/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/data/dict.csv')\n",
    "# len(list(word_dict[word_dict.Negative >0].Word))\n",
    "# len(list(word_dict[word_dict.Positive >0].Word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "word_dict = pd.read_csv('/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/master-dict/LoughranMcDonald_MasterDictionary_2016.csv')\n",
    "positive_txt = list(word_dict[word_dict.Positive >0].Word)\n",
    "negative_txt = list(word_dict[word_dict.Negative >0].Word)\n",
    "litigiuous_txt = list(word_dict[word_dict.Litigious >0].Word)\n",
    "uncertainty_txt = list(word_dict[word_dict.Uncertainty >0].Word)\n",
    "constaining_txt = list(word_dict[word_dict.Constraining >0].Word)\n",
    "superfluous_txt = list(word_dict[word_dict.Superfluous >0].Word)\n",
    "interesting_txt = list(word_dict[word_dict.Interesting >0].Word)\n",
    "modal_txt = list(word_dict[word_dict.Modal >0].Word)\n",
    "Irr_verb = list(word_dict[word_dict.Irr_Verb >0].Word)\n",
    "Harvard_IV = list(word_dict[word_dict.Harvard_IV >0].Word)\n",
    "# sentiments = ['Negative','Positive','Uncertainty','Litigious','Constraining','Superfluous',\n",
    "#               'Interesting','Modal','Irr_Verb','Harvard_IV']\n",
    "\n",
    "sent_list = [positive_txt, negative_txt, litigiuous_txt, uncertainty_txt, constaining_txt, superfluous_txt, interesting_txt, modal_txt, Irr_verb, Harvard_IV]\n",
    "for k in sent_list:\n",
    "    for i in range(len(k)):\n",
    "        k[i] = k[i].lower()\n",
    "#creating a dictionary of contractions \n",
    "\n",
    "contractions = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "#function for defining contraction's:\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    return text\n",
    "\n",
    "#function for removing unicode data :\n",
    "\n",
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "#function for removing all the scrub words\n",
    "def scrub_words(text):\n",
    "    #Replace \\xao characters in text\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    \n",
    "    #Replace non ascii / not words and digits\n",
    "    text = re.sub(\"(\\\\W|\\\\d)\",' ',text)\n",
    "    \n",
    "    #Replace new line characters and following text until space\n",
    "    text = re.sub('\\n(\\w*?)[\\s]', '', text)\n",
    "    \n",
    "    #Remove html markup\n",
    "    text = re.sub(\"<.*?>\", ' ', text)\n",
    "    \n",
    "    #Remove extra spaces from the text\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#list of stop words from the observations by reading some level-1 cleaned documents \n",
    "\n",
    "stopwords = ['dtd','copyright','llc','en','html','fe','ed','webfilings','e','vk','g','zip code', 'pagebreak','html' \n",
    "             'w','c','en','table','body','par','value','per','securities','exchange','comission','telephone','number',\n",
    "             'zip', 'code', 'end', 'page','xbrl','begin','dc','aa','aaa', 'aaa aa','ab','abn','abn amro','abnormal',\n",
    "             'abo','abs','ac','az','ba','baa','aoci','aol','apb','api','app','ann','anne','amp','amt','anda','bla','bit',\n",
    "             'bio','bhc','bb','bbb','bbl','bbls','bc','bcf','bcfe','apr','arc','aro','asa','asa','asc','asic','asp','asr',\n",
    "             'asu','asus','ave','bms','bnp','bny','boe','blvd','bms','boe','bps','bs','btu', 'btus','ca','cad','cal','ccc',\n",
    "             'cceeff','cdo','cdos','cds','ce','cede','cg','chk','cmsa','col','com','con','conway','ct','dd','de','dan',\n",
    "             'dana','dea','wti','wto','wv','wyeth','wyoming','xannual','xerox','xi','xii','xiii','xindicate','xiv','xix','xl',\n",
    "             'xthe','xv','xvi','xvii','xviii', 'xx','xxi','xxx','wi','vt','vs','von''vie','via','vi','var','ta','tab','tam',\n",
    "             'td','tdr','tdrs','te','sur','ss','sr','sq','sp','sop','sip','sd','sdn','se',\n",
    "             '__________________________________________ '\n",
    "            '__________ ']\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    \n",
    "    remove = re.sub(r'\\b\\w{1,3}\\b', '', raw_review) #removing all words less than 4 characters \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", remove) \n",
    "    word = letters_only.lower().split()\n",
    "  \n",
    "    meaningful_words = [w for w in word if not w in stopwords] \n",
    "    return( \" \".join(meaningful_words))\n",
    " \n",
    "#function for defining contraction's:\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    return text\n",
    "\n",
    "#function for removing unicode data :\n",
    "\n",
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "#function for removing all the scrub words\n",
    "def scrub_words(text):\n",
    "    #Replace \\xao characters in text\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    \n",
    "    #Replace non ascii / not words and digits\n",
    "    text = re.sub(\"(\\\\W|\\\\d)\",' ',text)\n",
    "    \n",
    "    #Replace new line characters and following text until space\n",
    "    text = re.sub('\\n(\\w*?)[\\s]', '', text)\n",
    "    \n",
    "    #Remove html markup\n",
    "    text = re.sub(\"<.*?>\", ' ', text)\n",
    "    \n",
    "    #Remove extra spaces from the text\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Let's define functions for creating new columns \n",
    "\n",
    "# Generic stop wordss list from the mcdonald weblink\\\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "stopWordsFile = '/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/StopWords_Generic.txt'\n",
    "with open(stopWordsFile ,'r') as stop_words:\n",
    "    stopWords = stop_words.read().lower()\n",
    "stopWordList = stopWords.split('\\n')\n",
    "stopWordList[-1:] = []\n",
    "# Tokenizer\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
    "    return filtered_words\n",
    "\n",
    "#===============================================================================\n",
    "# Loading positive words\n",
    "# positiveWordsFile = '/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/positive-words.txt'\n",
    "# with open(positiveWordsFile,'r') as posfile:\n",
    "#     positivewords=posfile.read().lower()\n",
    "positiveWordList=positive_txt\n",
    "\n",
    "# Calculating positive score \n",
    "def positive_score(text):\n",
    "    numPosWords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in positiveWordList:\n",
    "            numPosWords  += 1\n",
    "    \n",
    "    sumPos = numPosWords\n",
    "    return sumPos\n",
    "#===============================================================================\n",
    "# Loading negative words\n",
    "# nagitiveWordsFile = '/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/negative-words.txt'\n",
    "# with open(nagitiveWordsFile ,'r') as negfile:\n",
    "#     negativeword=negfile.read().lower()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity  # Value ranges from -1 (negative) to 1 (positive)\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "\n",
    "negativeWordList=negative_txt\n",
    "\n",
    "\n",
    "# Calculating Negative score\n",
    "def negative_word(text):\n",
    "    numNegWords=0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in negativeWordList:\n",
    "            numNegWords -=1\n",
    "    sumNeg = numNegWords \n",
    "    sumNeg = sumNeg * -1\n",
    "    return sumNeg\n",
    "#===============================================================================\n",
    "# Loading litigous words\n",
    "# litigous = '/content/litiguous.txt'\n",
    "# with open(litigous ,'r') as litfile:\n",
    "#     litigousword=litfile.read().lower()\n",
    "litigouswordlist =litigiuous_txt\n",
    "\n",
    "\n",
    "# Calculating litigous score \n",
    "def litigous_score(text):\n",
    "    numLitWords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in litigouswordlist:\n",
    "            numLitWords  += 1\n",
    "    \n",
    "    sumLit = numLitWords\n",
    "    return sumLit\n",
    "#===============================================================================\t\n",
    "# Loading interesting words\n",
    "# interestingWordsfile = '/content/interestingwords.txt'\n",
    "# with open(interestingWordsfile ,'r') as intfile:\n",
    "#     interestingWord=intfile.read().lower()\n",
    "interestingWordList=interesting_txt\n",
    "\n",
    "\n",
    "# Calculating interestingword score \n",
    "def interesting_score(text):\n",
    "    numintwords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in interestingWordList:\n",
    "            numintwords  += 1\n",
    "    \n",
    "    sumInt = numintwords\n",
    "    return sumInt\n",
    "\n",
    "#===============================================================================\n",
    "# Loading superflous words\n",
    "# superflousfile = '/content/superfluous.txt'\n",
    "# with open(superflousfile ,'r') as supfile:\n",
    "#     supword=supfile.read().lower()\n",
    "supWordList=superfluous_txt\n",
    "\n",
    "\n",
    "# Calculating sup_score \n",
    "def sup_score(text):\n",
    "    numSupWords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in supWordList:\n",
    "            numSupWords  += 1\n",
    "\n",
    "         \n",
    "    sumSup = numSupWords\n",
    "    return sumSup\n",
    "#===============================================================================\n",
    "# Loading Modalstrong words\n",
    "# modalstrongfile = '/content/modal_strong.txt'\n",
    "# with open(modalstrongfile ,'r') as modfile:\n",
    "#     modstrongword=modfile.read().lower()\n",
    "modstrongwordlist=modal_txt\n",
    "\n",
    "\n",
    "# Calculating modal strongscore \n",
    "def modal_strong_store(text):\n",
    "    nomodstr = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in modstrongwordlist:\n",
    "            nomodstr  += 1\n",
    "    \n",
    "    summodstr = nomodstr\n",
    "    return summodstr\n",
    "#===============================================================================\t\n",
    "# # Loading Modalneutral words\n",
    "# modalneutralfile = '/content/modal_neutral.txt'\n",
    "# with open(modalneutralfile ,'r') as modn:\n",
    "#     modneuword=modn.read().lower()\n",
    "# modneuwordlist=modneuword.split('\\n')\n",
    "\n",
    "\n",
    "# # Calculating modal_neu_score\n",
    "# def modal_neutral_score(text):\n",
    "#     nomodneu = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in modneuwordlist:\n",
    "#             nomodneu  += 1\n",
    "    \n",
    "#     summodneu = nomodneu\n",
    "#     return summodneu\n",
    "\n",
    "# #===============================================================================\n",
    "# # Loading modal_weak words\n",
    "# modal_weak_file = '/content/modal_weak.txt'\n",
    "# with open(modal_weak_file ,'r') as modweak:\n",
    "#     modweakword=modweak.read().lower()\n",
    "# modweaklist=modweakword.split('\\n')\n",
    "\n",
    "\n",
    "# # Calculating mod_weak_score \n",
    "# def mod_weak_score(text):\n",
    "#     nomodweak = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in modweaklist:\n",
    "#             nomodweak  += 1\n",
    "    \n",
    "#     summodweak = nomodweak\n",
    "#     return summodweak\n",
    "\t\n",
    "#===============================================================================\n",
    "# # Loading dictionary_countcount words\n",
    "# # dictionary_wordfile = '/content/dictionary.txt'\n",
    "# # with open(dictionary_wordfile ,'r') as dicfile:\n",
    "# #     dicword=dicfile.read().lower()\n",
    "# dicwordlist=\n",
    "\n",
    "\n",
    "# # Calculating dic_word_score score \n",
    "# def dic_word_score(text):\n",
    "#     nodicwords = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in dicwordlist:\n",
    "#             nodicwords  += 1\n",
    "    \n",
    "#     sumdic = nodickwords\n",
    "#     return sumdic\n",
    "#===============================================================================\n",
    "\n",
    "# Loading harvard_1 words\n",
    "# harvard_1_file = '/content/harvard_1.txt'\n",
    "# with open(harvard_1_file ,'r') as harfile:\n",
    "#     har_1_word=harfile.read().lower()\n",
    "har_1_list=Harvard_IV\n",
    "\n",
    "\n",
    "# Calculating dic_word_score score \n",
    "def har_1_score(text):\n",
    "    nohar_1_words = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in har_1_list:\n",
    "            nohar_1_words  += 1\n",
    "    \n",
    "    sumhar_1 = nohar_1_words\n",
    "    return sumhar_1\n",
    "#===============================================================================\n",
    "# Calculating polarity score\n",
    "def polarity_score(positiveScore, negativeScore):\n",
    "    pol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)\n",
    "    return pol_score\n",
    "\n",
    "#===============================================================================\n",
    "# Calculating Average sentence length \n",
    "\n",
    "'''# It will calculated using formula --- \n",
    "      Average Sentence Length = the number of words / the number of sentences'''\n",
    "     \n",
    "def average_sentence_length(text):\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    tokens = tokenizer(text)\n",
    "    totalWordCount = len(tokens)\n",
    "    totalSentences = len(sentence_list)\n",
    "    average_sent = 0\n",
    "    if totalSentences != 0:\n",
    "        average_sent = totalWordCount / totalSentences\n",
    "    \n",
    "    average_sent_length= average_sent\n",
    "    \n",
    "    return round(average_sent_length)\n",
    "#===============================================================================\n",
    "# Calculating percentage of complex words\n",
    "\n",
    "''' It is calculated using Percentage of \n",
    "            Complex words = the number of complex words / the number of words''' \n",
    "\n",
    "def percentage_complex_word(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage\n",
    "#===============================================================================\n",
    "\n",
    "# calculating Fog Index \n",
    "\n",
    "'''# Fog index is calculated using --\n",
    "    Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)'''\n",
    "\n",
    "def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return fogIndex\n",
    "\n",
    "#===============================================================================\n",
    "# Counting complex words\n",
    "def complex_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    return complexWord\n",
    "#===============================================================================\n",
    "#Counting total words\n",
    "\n",
    "def total_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens)\n",
    "\n",
    "#===============================================================================\n",
    "# calculating uncertainty_score\n",
    "# uncertainty_dictionaryFile= '/content/uncertainity.txt'\n",
    "# with open(uncertainty_dictionaryFile ,'r') as uncertain_dict:\n",
    "#     uncertainDict=uncertain_dict.read().lower()\n",
    "uncertainDictionary = uncertainty_txt\n",
    "\n",
    "#uncertainity score \n",
    "def uncertainty_score(text):\n",
    "    uncertainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in uncertainDictionary:\n",
    "            uncertainWordnum +=1\n",
    "    sumUncertainityScore = uncertainWordnum \n",
    "    \n",
    "    return sumUncertainityScore\n",
    "#===============================================================================\n",
    "# calculating constraining score\n",
    "# constraining_dictionaryFile = '/content/constrained.txt'\n",
    "# with open(constraining_dictionaryFile ,'r') as constraining_dict:\n",
    "#     constrainDict=constraining_dict.read().lower()\n",
    "constrainDictionary = constaining_txt\n",
    "\n",
    "\n",
    "def constraining_score(text):\n",
    "    constrainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnum +=1\n",
    "    sumConstrainScore = constrainWordnum \n",
    "    \n",
    "    return sumConstrainScore\n",
    "#===============================================================================\n",
    "# Calculating positive word proportion\n",
    "def positive_word_prop(positiveScore,wordcount):\n",
    "    positive_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        positive_word_proportion = positiveScore / wordcount\n",
    "        \n",
    "    return positive_word_proportion\n",
    "\n",
    "#===============================================================================\n",
    "# Calculating negative word proportion\n",
    "\n",
    "def negative_word_prop(negativeScore,wordcount):\n",
    "    negative_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        negative_word_proportion = negativeScore / wordcount\n",
    "        \n",
    "    return negative_word_proportion\n",
    "#===============================================================================\n",
    "# Calculating uncertain word proportion\n",
    "def uncertain_word_prop(uncertainScore,wordcount):\n",
    "    uncertain_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "    return uncertain_word_proportion\n",
    "\n",
    "#===============================================================================\n",
    "# Calculating uncertain word proportion\n",
    "def uncertain_word_prop(uncertainScore,wordcount):\n",
    "    uncertain_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "    return uncertain_word_proportion\n",
    "\n",
    "#===============================================================================\n",
    "# Calculating constraining word proportion\n",
    "def constraining_word_prop(constrainingScore,wordcount):\n",
    "    constraining_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        constraining_word_proportion = constrainingScore / wordcount\n",
    "        \n",
    "    return constraining_word_proportion\n",
    "#===============================================================================\n",
    "# calculating Constraining words for whole report\n",
    "def constrain_word_whole(mdaText):\n",
    "    wholeDoc = mdaText\n",
    "    constrainWordnumWhole =0\n",
    "    rawToken = tokenizer(wholeDoc)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnumWhole +=1\n",
    "    sumConstrainScoreWhole = constrainWordnumWhole \n",
    "    \n",
    "    return sumConstrainScoreWhole\n",
    "#===============================================================================\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>company</th>\n",
       "      <th>litigous_score</th>\n",
       "      <th>superfluous_score</th>\n",
       "      <th>interesting_score</th>\n",
       "      <th>modal_score</th>\n",
       "      <th>harvard_1</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>avg_sen_length</th>\n",
       "      <th>fog_index</th>\n",
       "      <th>Future Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.017580</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.014124</td>\n",
       "      <td>0.047509</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>33202</td>\n",
       "      <td>13280.973893</td>\n",
       "      <td>0.036627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.048322</td>\n",
       "      <td>0.051530</td>\n",
       "      <td>32329</td>\n",
       "      <td>12931.773862</td>\n",
       "      <td>-0.242095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.013058</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.011426</td>\n",
       "      <td>0.053947</td>\n",
       "      <td>0.057735</td>\n",
       "      <td>46143</td>\n",
       "      <td>18457.367887</td>\n",
       "      <td>0.085822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.012558</td>\n",
       "      <td>0.057231</td>\n",
       "      <td>0.061721</td>\n",
       "      <td>37799</td>\n",
       "      <td>15119.770491</td>\n",
       "      <td>0.362164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.012938</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.012615</td>\n",
       "      <td>0.058108</td>\n",
       "      <td>0.064072</td>\n",
       "      <td>38732</td>\n",
       "      <td>15492.970464</td>\n",
       "      <td>-0.026202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.013071</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.013307</td>\n",
       "      <td>0.056927</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>36912</td>\n",
       "      <td>14764.973992</td>\n",
       "      <td>0.325174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.017205</td>\n",
       "      <td>0.042083</td>\n",
       "      <td>0.045054</td>\n",
       "      <td>21236</td>\n",
       "      <td>8494.584065</td>\n",
       "      <td>0.427630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.010144</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.017583</td>\n",
       "      <td>0.044043</td>\n",
       "      <td>0.044256</td>\n",
       "      <td>20623</td>\n",
       "      <td>8249.385967</td>\n",
       "      <td>0.341451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.045590</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>20292</td>\n",
       "      <td>8116.984073</td>\n",
       "      <td>0.099274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.012631</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.019142</td>\n",
       "      <td>0.046903</td>\n",
       "      <td>0.047337</td>\n",
       "      <td>22332</td>\n",
       "      <td>8932.981426</td>\n",
       "      <td>0.533768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.049623</td>\n",
       "      <td>0.046973</td>\n",
       "      <td>22006</td>\n",
       "      <td>8802.581550</td>\n",
       "      <td>-0.257112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.019629</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.041190</td>\n",
       "      <td>22623</td>\n",
       "      <td>9049.381815</td>\n",
       "      <td>0.565718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2014</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.015840</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.053188</td>\n",
       "      <td>15527</td>\n",
       "      <td>6210.976029</td>\n",
       "      <td>-0.072141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.014999</td>\n",
       "      <td>0.049593</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>14881</td>\n",
       "      <td>5952.577085</td>\n",
       "      <td>1.343784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.051843</td>\n",
       "      <td>0.059144</td>\n",
       "      <td>15498</td>\n",
       "      <td>6199.376255</td>\n",
       "      <td>0.082357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.052055</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>14747</td>\n",
       "      <td>5898.977419</td>\n",
       "      <td>0.550565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.008501</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.014792</td>\n",
       "      <td>0.053753</td>\n",
       "      <td>0.059575</td>\n",
       "      <td>15823</td>\n",
       "      <td>6329.379561</td>\n",
       "      <td>0.394353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>0.053841</td>\n",
       "      <td>0.053331</td>\n",
       "      <td>15553</td>\n",
       "      <td>6221.380493</td>\n",
       "      <td>0.208884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>0.056858</td>\n",
       "      <td>0.052366</td>\n",
       "      <td>17804</td>\n",
       "      <td>7121.778589</td>\n",
       "      <td>-0.221771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.012721</td>\n",
       "      <td>0.058703</td>\n",
       "      <td>0.051306</td>\n",
       "      <td>19099</td>\n",
       "      <td>7639.780617</td>\n",
       "      <td>1.177831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>0.056813</td>\n",
       "      <td>0.058526</td>\n",
       "      <td>18850</td>\n",
       "      <td>7540.181178</td>\n",
       "      <td>0.109456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>0.052101</td>\n",
       "      <td>18871</td>\n",
       "      <td>7548.580234</td>\n",
       "      <td>0.559564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.012871</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.053431</td>\n",
       "      <td>0.053791</td>\n",
       "      <td>19665</td>\n",
       "      <td>7866.181073</td>\n",
       "      <td>0.284317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.012524</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.013132</td>\n",
       "      <td>0.054081</td>\n",
       "      <td>0.052896</td>\n",
       "      <td>18299</td>\n",
       "      <td>7319.781168</td>\n",
       "      <td>0.230277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year              company  litigous_score  superfluous_score  \\\n",
       "0   2014  AMERICAN EXPRESS CO        0.017580           0.000224   \n",
       "1   2015  AMERICAN EXPRESS CO        0.018550           0.000172   \n",
       "2   2016  AMERICAN EXPRESS CO        0.013058           0.000177   \n",
       "3   2017  AMERICAN EXPRESS CO        0.013551           0.000149   \n",
       "4   2018  AMERICAN EXPRESS CO        0.012938           0.000081   \n",
       "5   2019  AMERICAN EXPRESS CO        0.013071           0.000118   \n",
       "6   2014         Facebook Inc        0.010787           0.000109   \n",
       "7   2015         Facebook Inc        0.010144           0.000141   \n",
       "8   2016         Facebook Inc        0.010719           0.000114   \n",
       "9   2017         Facebook Inc        0.012631           0.000104   \n",
       "10  2018         Facebook Inc        0.011533           0.000132   \n",
       "11  2019         Facebook Inc        0.012075           0.000360   \n",
       "12  2014          NETFLIX INC        0.008947           0.000194   \n",
       "13  2015          NETFLIX INC        0.009717           0.000242   \n",
       "14  2016          NETFLIX INC        0.009877           0.000312   \n",
       "15  2017          NETFLIX INC        0.007876           0.000328   \n",
       "16  2018          NETFLIX INC        0.008501           0.000343   \n",
       "17  2019          NETFLIX INC        0.009433           0.000272   \n",
       "18  2014       AMAZON COM INC        0.014040           0.000035   \n",
       "19  2015       AMAZON COM INC        0.014278           0.000032   \n",
       "20  2016       AMAZON COM INC        0.012388           0.000033   \n",
       "21  2017       AMAZON COM INC        0.013163           0.000033   \n",
       "22  2018       AMAZON COM INC        0.012871           0.000031   \n",
       "23  2019       AMAZON COM INC        0.012524           0.000034   \n",
       "\n",
       "    interesting_score  modal_score  harvard_1  polarity_score avg_sen_length  \\\n",
       "0            0.001607     0.014124   0.047509        0.053458          33202   \n",
       "1            0.001603     0.014447   0.048322        0.051530          32329   \n",
       "2            0.001374     0.011426   0.053947        0.057735          46143   \n",
       "3            0.001588     0.012558   0.057231        0.061721          37799   \n",
       "4            0.001629     0.012615   0.058108        0.064072          38732   \n",
       "5            0.001368     0.013307   0.056927        0.065217          36912   \n",
       "6            0.001775     0.017205   0.042083        0.045054          21236   \n",
       "7            0.001803     0.017583   0.044043        0.044256          20623   \n",
       "8            0.001686     0.018550   0.045590        0.046100          20292   \n",
       "9            0.001641     0.019142   0.046903        0.047337          22332   \n",
       "10           0.001561     0.020368   0.049623        0.046973          22006   \n",
       "11           0.001593     0.019629   0.050100        0.041190          22623   \n",
       "12           0.002517     0.015840   0.048567        0.053188          15527   \n",
       "13           0.002903     0.014999   0.049593        0.056433          14881   \n",
       "14           0.003006     0.014327   0.051843        0.059144          15498   \n",
       "15           0.002995     0.015301   0.052055        0.061659          14747   \n",
       "16           0.001830     0.014792   0.053753        0.059575          15823   \n",
       "17           0.001747     0.014984   0.053841        0.053331          15553   \n",
       "18           0.001498     0.012124   0.056858        0.052366          17804   \n",
       "19           0.001623     0.012721   0.058703        0.051306          19099   \n",
       "20           0.001610     0.013242   0.056813        0.058526          18850   \n",
       "21           0.001313     0.013196   0.055968        0.052101          18871   \n",
       "22           0.001632     0.013091   0.053431        0.053791          19665   \n",
       "23           0.001654     0.013132   0.054081        0.052896          18299   \n",
       "\n",
       "       fog_index  Future Return  \n",
       "0   13280.973893       0.036627  \n",
       "1   12931.773862      -0.242095  \n",
       "2   18457.367887       0.085822  \n",
       "3   15119.770491       0.362164  \n",
       "4   15492.970464      -0.026202  \n",
       "5   14764.973992       0.325174  \n",
       "6    8494.584065       0.427630  \n",
       "7    8249.385967       0.341451  \n",
       "8    8116.984073       0.099274  \n",
       "9    8932.981426       0.533768  \n",
       "10   8802.581550      -0.257112  \n",
       "11   9049.381815       0.565718  \n",
       "12   6210.976029      -0.072141  \n",
       "13   5952.577085       1.343784  \n",
       "14   6199.376255       0.082357  \n",
       "15   5898.977419       0.550565  \n",
       "16   6329.379561       0.394353  \n",
       "17   6221.380493       0.208884  \n",
       "18   7121.778589      -0.221771  \n",
       "19   7639.780617       1.177831  \n",
       "20   7540.181178       0.109456  \n",
       "21   7548.580234       0.559564  \n",
       "22   7866.181073       0.284317  \n",
       "23   7319.781168       0.230277  "
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rishikumra/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMERICAN EXPRESS CO 2014\n",
      "AMERICAN EXPRESS CO 2015\n",
      "AMERICAN EXPRESS CO 2016\n",
      "AMERICAN EXPRESS CO 2017\n",
      "AMERICAN EXPRESS CO 2018\n",
      "AMERICAN EXPRESS CO 2019\n",
      "Facebook Inc 2014\n",
      "Facebook Inc 2015\n",
      "Facebook Inc 2016\n",
      "Facebook Inc 2017\n",
      "Facebook Inc 2018\n",
      "Facebook Inc 2019\n",
      "NETFLIX INC 2014\n",
      "NETFLIX INC 2015\n",
      "NETFLIX INC 2016\n",
      "NETFLIX INC 2017\n",
      "NETFLIX INC 2018\n",
      "NETFLIX INC 2019\n",
      "AMAZON COM INC 2014\n",
      "AMAZON COM INC 2015\n",
      "AMAZON COM INC 2016\n",
      "AMAZON COM INC 2017\n",
      "AMAZON COM INC 2018\n",
      "AMAZON COM INC 2019\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "final_df = pd.DataFrame(columns = ['year', 'company', 'positive_score', 'negative_score', 'litigous_score', 'superfluous_score', 'interesting_score', 'modal_score', 'harvard_1', 'polarity_score', 'avg_sen_length', 'fog_index'])\n",
    "company_name_list = ['AMERICAN EXPRESS CO', 'MICROSOFT CORP', 'Facebook Inc', 'NETFLIX INC', 'AMAZON COM INC']\n",
    "for comp_curr in company_name_list:\n",
    "    company_dir = os.path.join(project_dir, 'sec-filings-downloaded', comp_curr)\n",
    "    try:\n",
    "        os.chdir(os.path.join(company_dir, 'cleaned_filings'))\n",
    "    except:\n",
    "        continue\n",
    "    ten_k_dict = {}\n",
    "    ten_q_dict = {}\n",
    "    \n",
    "    for file in os.listdir():\n",
    "        if file.endswith('10-K'): \n",
    "            filing_year = int(file[8:12])\n",
    "            ten_k_dict[filing_year] = file\n",
    "\n",
    "    years_ = [2014,2015,2016,2017,2018,2019]\n",
    "    for max_ten_k_year in years_:\n",
    "        # try:\n",
    "            with open(ten_k_dict[max_ten_k_year]) as file:\n",
    "                latest_ten_k = file.readline()\n",
    "            if latest_ten_k!=None:\n",
    "                print(comp_curr, max_ten_k_year)\n",
    "            #using contractions dictionary to make corrections \n",
    "                latest_ten_k = expand_contractions(re.sub('', \"'\", latest_ten_k))\n",
    "\n",
    "                #stripping the words using space \n",
    "                latest_ten_k = latest_ten_k.strip().lower()\n",
    "\n",
    "                #removing accented characters \n",
    "                latest_ten_k = remove_accented_chars(latest_ten_k)\n",
    "\n",
    "                #re-placing \" \" \" with space \n",
    "                latest_ten_k = latest_ten_k.replace('\"', '')\n",
    "\n",
    "                # Removing url's from the text\n",
    "                url_reg  = r'[a-z]*[:.]+\\S+'\n",
    "                latest_ten_k = re.sub(url_reg, '', latest_ten_k)\n",
    "\n",
    "                latest_ten_k = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", latest_ten_k)\n",
    "\n",
    "                #removing scrub_words\n",
    "                latest_ten_k = scrub_words(latest_ten_k)\n",
    "\n",
    "                #replace spaaces more than one with single space \n",
    "                latest_ten_k = re.sub(\"\\s+\", ' ', latest_ten_k)\n",
    "\n",
    "                #finding the length of the words in the data\n",
    "                word_count = len(latest_ten_k.split(' '))\n",
    "                        \n",
    "                data = review_to_words(latest_ten_k)\n",
    "\n",
    "                positive_scor = positive_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "                negative_scor = negative_word(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "                litigous_scor = litigous_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "                superfluous_scor = sup_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "                interesting_socr = interesting_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "                modal_scor = modal_strong_store(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "                harvard_1 = har_1_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "                polarity_scor = polarity_score(positive_scor, negative_scor)\n",
    "                polar_ = analyze_sentiment(latest_ten_k)\n",
    "                avg_sen_leng = average_sentence_length(latest_ten_k)\n",
    "                fog_inde = fog_index(avg_sen_leng, percentage_complex_word(latest_ten_k))\n",
    "\n",
    "                new_row = pd.DataFrame({\n",
    "                    'year': [max_ten_k_year],\n",
    "                    'company': [comp_curr],\n",
    "                    'positive_score': [positive_scor],\n",
    "                    'negative_score': [negative_scor],\n",
    "                    'litigous_score': [litigous_scor],\n",
    "                    'superfluous_score': [superfluous_scor],\n",
    "                    'interesting_score': [interesting_socr],\n",
    "                    'modal_score': [modal_scor],\n",
    "                    'harvard_1': [harvard_1],\n",
    "                    'polarity_score': [polar_],\n",
    "                    'avg_sen_length': [avg_sen_leng],\n",
    "                    'fog_index': [fog_inde]\n",
    "                })\n",
    "                final_df = pd.concat([final_df, new_row], ignore_index=False)\n",
    "                # df = final_df.append({'year': max_ten_k_year, 'company': comp_curr, 'positive_score': positive_score}, ignore_index=True)\n",
    "                # df\n",
    "        # except:\n",
    "        #     print('error for', comp_curr)\n",
    "            \n",
    "final_df.index = range(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(columns=['positive_score', 'negative_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2015: 'cleaned_2015-02-24_10-K',\n",
       " 2017: 'cleaned_2017-02-17_10-K',\n",
       " 2018: 'cleaned_2018-02-16_10-K',\n",
       " 2019: 'cleaned_2019-02-13_10-K',\n",
       " 2014: 'cleaned_2014-02-25_10-K',\n",
       " 2016: 'cleaned_2016-02-19_10-K'}"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # # company_dir = os.path.join(project_dir, 'sec-filings-downloaded', 'AMERICAN EXPRESS CO')\n",
    "# # # # os.chdir(os.path.join(company_dir, 'cleaned_filings'))\n",
    "\n",
    "# # # # ten_k_dict = {}\n",
    "# # # # ten_q_dict = {}\n",
    "\n",
    "# # # # for file in os.listdir():\n",
    "# # # #     if file.endswith('10-K'): \n",
    "# # # #         filing_year = int(file[8:12])\n",
    "# # # #         ten_k_dict[filing_year] = file\n",
    "\n",
    "# # # # ten_k_dict\n",
    "# # # # import nltk\n",
    "# # # # nltk.download('punkt_tab')\n",
    "# # # # max_ten_k_year = max(ten_k_dict, key=ten_k_dict.get)\n",
    "# # # # year_before_max_ten_k = max_ten_k_year - 1\n",
    "# # # # max_ten_k_year = 2015\n",
    "\n",
    "# # # # with open(ten_k_dict[max_ten_k_year]) as file:\n",
    "# # # #     latest_ten_k = file.readline()\n",
    "# # # # with open(ten_k_dict[year_before_max_ten_k]) as file:\n",
    "# # # #     previous_ten_k = file.readline()\n",
    "\n",
    "# # # # ten_k_vec = vectorize_and_preprocess_filings([latest_ten_k, previous_ten_k])\n",
    "# # # # cosine_sim_ten_k = calculate_consine_similarity(ten_k_vec.toarray()[0], ten_k_vec.toarray()[1])\n",
    "# # # # new_row = {\n",
    "# # # #     'company': company, \n",
    "# # # #     'cosine_similarity': cosine_sim_ten_k,\n",
    "# # # #     'latest_filing_dt': ten_k_dict[max_ten_k_year][8:18],\n",
    "# # # #     'previous_filing_dt': ten_k_dict[year_before_max_ten_k][8:18]\n",
    "# # # # }\n",
    "# # # # df_ten_k_results = pd.concat([df_ten_k_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# # # # with open(ten_k_dict[max_ten_k_year]) as file:\n",
    "# # # #     latest_ten_k = file.readline()\n",
    "# # # # word_dict = pd.read_csv('/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/master-dict/LoughranMcDonald_MasterDictionary_2016.csv')\n",
    "\n",
    "# # # # positive_txt = list(word_dict[word_dict.Positive >0].Word)\n",
    "# # # # negative_txt = list(word_dict[word_dict.Negative >0].Word)\n",
    "# # # # litigiuous_txt = list(word_dict[word_dict.Litigious >0].Word)\n",
    "# # # # uncertainty_txt = list(word_dict[word_dict.Uncertainty >0].Word)\n",
    "# # # # constaining_txt = list(word_dict[word_dict.Constraining >0].Word)\n",
    "# # # # superfluous_txt = list(word_dict[word_dict.Superfluous >0].Word)\n",
    "# # # # interesting_txt = list(word_dict[word_dict.Interesting >0].Word)\n",
    "# # # # modal_txt = list(word_dict[word_dict.Modal >0].Word)\n",
    "# # # # Irr_verb = list(word_dict[word_dict.Irr_Verb >0].Word)\n",
    "# # # # Harvard_IV = list(word_dict[word_dict.Harvard_IV >0].Word)\n",
    "# # # # # sentiments = ['Negative','Positive','Uncertainty','Litigious','Constraining','Superfluous',\n",
    "# # # # #               'Interesting','Modal','Irr_Verb','Harvard_IV']\n",
    "\n",
    "# # # # sent_list = [positive_txt, negative_txt, litigiuous_txt, uncertainty_txt, constaining_txt, superfluous_txt, interesting_txt, modal_txt, Irr_verb, Harvard_IV]\n",
    "\n",
    "# # # word_dict\n",
    "# # # sentiments = ['Negative','Positive','Uncertainty','Litigious','Constraining','Superfluous',\n",
    "# # #               'Interesting','Modal','Irr_Verb','Harvard_IV']\n",
    "# # # #creating a dictionary of contractions \n",
    "\n",
    "# # # contractions = {\n",
    "# # # \"ain't\": \"is not\",\n",
    "# # # \"aren't\": \"are not\",\n",
    "# # # \"can't\": \"cannot\",\n",
    "# # # \"can't've\": \"cannot have\",\n",
    "# # # \"'cause\": \"because\",\n",
    "# # # \"could've\": \"could have\",\n",
    "# # # \"couldn't\": \"could not\",\n",
    "# # # \"couldn't've\": \"could not have\",\n",
    "# # # \"didn't\": \"did not\",\n",
    "# # # \"doesn't\": \"does not\",\n",
    "# # # \"don't\": \"do not\",\n",
    "# # # \"hadn't\": \"had not\",\n",
    "# # # \"hadn't've\": \"had not have\",\n",
    "# # # \"hasn't\": \"has not\",\n",
    "# # # \"haven't\": \"have not\",\n",
    "# # # \"he'd\": \"he would\",\n",
    "# # # \"he'd've\": \"he would have\",\n",
    "# # # \"he'll\": \"he will\",\n",
    "# # # \"he'll've\": \"he he will have\",\n",
    "# # # \"he's\": \"he is\",\n",
    "# # # \"how'd\": \"how did\",\n",
    "# # # \"how'd'y\": \"how do you\",\n",
    "# # # \"how'll\": \"how will\",\n",
    "# # # \"how's\": \"how is\",\n",
    "# # # \"I'd\": \"I would\",\n",
    "# # # \"I'd've\": \"I would have\",\n",
    "# # # \"I'll\": \"I will\",\n",
    "# # # \"I'll've\": \"I will have\",\n",
    "# # # \"I'm\": \"I am\",\n",
    "# # # \"I've\": \"I have\",\n",
    "# # # \"i'd\": \"i would\",\n",
    "# # # \"i'd've\": \"i would have\",\n",
    "# # # \"i'll\": \"i will\",\n",
    "# # # \"i'll've\": \"i will have\",\n",
    "# # # \"i'm\": \"i am\",\n",
    "# # # \"i've\": \"i have\",\n",
    "# # # \"isn't\": \"is not\",\n",
    "# # # \"it'd\": \"it would\",\n",
    "# # # \"it'd've\": \"it would have\",\n",
    "# # # \"it'll\": \"it will\",\n",
    "# # # \"it'll've\": \"it will have\",\n",
    "# # # \"it's\": \"it is\",\n",
    "# # # \"let's\": \"let us\",\n",
    "# # # \"ma'am\": \"madam\",\n",
    "# # # \"mayn't\": \"may not\",\n",
    "# # # \"might've\": \"might have\",\n",
    "# # # \"mightn't\": \"might not\",\n",
    "# # # \"mightn't've\": \"might not have\",\n",
    "# # # \"must've\": \"must have\",\n",
    "# # # \"mustn't\": \"must not\",\n",
    "# # # \"mustn't've\": \"must not have\",\n",
    "# # # \"needn't\": \"need not\",\n",
    "# # # \"needn't've\": \"need not have\",\n",
    "# # # \"o'clock\": \"of the clock\",\n",
    "# # # \"oughtn't\": \"ought not\",\n",
    "# # # \"oughtn't've\": \"ought not have\",\n",
    "# # # \"shan't\": \"shall not\",\n",
    "# # # \"sha'n't\": \"shall not\",\n",
    "# # # \"shan't've\": \"shall not have\",\n",
    "# # # \"she'd\": \"she would\",\n",
    "# # # \"she'd've\": \"she would have\",\n",
    "# # # \"she'll\": \"she will\",\n",
    "# # # \"she'll've\": \"she will have\",\n",
    "# # # \"she's\": \"she is\",\n",
    "# # # \"should've\": \"should have\",\n",
    "# # # \"shouldn't\": \"should not\",\n",
    "# # # \"shouldn't've\": \"should not have\",\n",
    "# # # \"so've\": \"so have\",\n",
    "# # # \"so's\": \"so as\",\n",
    "# # # \"that'd\": \"that would\",\n",
    "# # # \"that'd've\": \"that would have\",\n",
    "# # # \"that's\": \"that is\",\n",
    "# # # \"there'd\": \"there would\",\n",
    "# # # \"there'd've\": \"there would have\",\n",
    "# # # \"there's\": \"there is\",\n",
    "# # # \"they'd\": \"they would\",\n",
    "# # # \"they'd've\": \"they would have\",\n",
    "# # # \"they'll\": \"they will\",\n",
    "# # # \"they'll've\": \"they will have\",\n",
    "# # # \"they're\": \"they are\",\n",
    "# # # \"they've\": \"they have\",\n",
    "# # # \"to've\": \"to have\",\n",
    "# # # \"wasn't\": \"was not\",\n",
    "# # # \"we'd\": \"we would\",\n",
    "# # # \"we'd've\": \"we would have\",\n",
    "# # # \"we'll\": \"we will\",\n",
    "# # # \"we'll've\": \"we will have\",\n",
    "# # # \"we're\": \"we are\",\n",
    "# # # \"we've\": \"we have\",\n",
    "# # # \"weren't\": \"were not\",\n",
    "# # # \"what'll\": \"what will\",\n",
    "# # # \"what'll've\": \"what will have\",\n",
    "# # # \"what're\": \"what are\",\n",
    "# # # \"what's\": \"what is\",\n",
    "# # # \"what've\": \"what have\",\n",
    "# # # \"when's\": \"when is\",\n",
    "# # # \"when've\": \"when have\",\n",
    "# # # \"where'd\": \"where did\",\n",
    "# # # \"where's\": \"where is\",\n",
    "# # # \"where've\": \"where have\",\n",
    "# # # \"who'll\": \"who will\",\n",
    "# # # \"who'll've\": \"who will have\",\n",
    "# # # \"who's\": \"who is\",\n",
    "# # # \"who've\": \"who have\",\n",
    "# # # \"why's\": \"why is\",\n",
    "# # # \"why've\": \"why have\",\n",
    "# # # \"will've\": \"will have\",\n",
    "# # # \"won't\": \"will not\",\n",
    "# # # \"won't've\": \"will not have\",\n",
    "# # # \"would've\": \"would have\",\n",
    "# # # \"wouldn't\": \"would not\",\n",
    "# # # \"wouldn't've\": \"would not have\",\n",
    "# # # \"y'all\": \"you all\",\n",
    "# # # \"y'all'd\": \"you all would\",\n",
    "# # # \"y'all'd've\": \"you all would have\",\n",
    "# # # \"y'all're\": \"you all are\",\n",
    "# # # \"y'all've\": \"you all have\",\n",
    "# # # \"you'd\": \"you would\",\n",
    "# # # \"you'd've\": \"you would have\",\n",
    "# # # \"you'll\": \"you will\",\n",
    "# # # \"you'll've\": \"you will have\",\n",
    "# # # \"you're\": \"you are\",\n",
    "# # # \"you've\": \"you have\"\n",
    "# # # }\n",
    "# # #function for defining contraction's:\n",
    "\n",
    "# # def expand_contractions(text):\n",
    "# #     for word in text.split():\n",
    "# #         if word.lower() in contractions:\n",
    "# #             text = text.replace(word, contractions[word.lower()])\n",
    "# #     return text\n",
    "\n",
    "# # #function for removing unicode data :\n",
    "\n",
    "# # import unicodedata\n",
    "# # def remove_accented_chars(text):\n",
    "# #     text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "# #     return text\n",
    "\n",
    "# # #function for removing all the scrub words\n",
    "# # def scrub_words(text):\n",
    "# #     #Replace \\xao characters in text\n",
    "# #     text = re.sub('\\xa0', ' ', text)\n",
    "    \n",
    "# #     #Replace non ascii / not words and digits\n",
    "# #     text = re.sub(\"(\\\\W|\\\\d)\",' ',text)\n",
    "    \n",
    "# #     #Replace new line characters and following text until space\n",
    "# #     text = re.sub('\\n(\\w*?)[\\s]', '', text)\n",
    "    \n",
    "# #     #Remove html markup\n",
    "# #     text = re.sub(\"<.*?>\", ' ', text)\n",
    "    \n",
    "# #     #Remove extra spaces from the text\n",
    "# #     text = re.sub(\"\\s+\", ' ', text)\n",
    "# #     return text\n",
    "\n",
    "\n",
    "# # #Let's define functions for creating new columns \n",
    "\n",
    "# # # Generic stop wordss list from the mcdonald weblink\\\n",
    "# # from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# # stopWordsFile = '/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/StopWords_Generic.txt'\n",
    "# # with open(stopWordsFile ,'r') as stop_words:\n",
    "# #     stopWords = stop_words.read().lower()\n",
    "# # stopWordList = stopWords.split('\\n')\n",
    "# # stopWordList[-1:] = []\n",
    "# # # Tokenizer\n",
    "# # def tokenizer(text):\n",
    "# #     text = text.lower()\n",
    "# #     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# #     tokens = tokenizer.tokenize(text)\n",
    "# #     filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
    "# #     return filtered_words\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # Loading positive words\n",
    "# # positiveWordList=positive_txt\n",
    "\n",
    "# # # Calculating positive score \n",
    "# # def positive_score(text):\n",
    "# #     numPosWords = 0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in positiveWordList:\n",
    "# #             numPosWords  += 1\n",
    "    \n",
    "# #     sumPos = numPosWords\n",
    "# #     return sumPos\n",
    "# # #===============================================================================\n",
    "# # # Loading negative words\n",
    "# # # nagitiveWordsFile = '/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/negative-words.txt'\n",
    "# # # with open(nagitiveWordsFile ,'r') as negfile:\n",
    "# # #     negativeword=negfile.read().lower()\n",
    "# # negativeWordList=negative_txt\n",
    "\n",
    "\n",
    "# # # Calculating Negative score\n",
    "# # def negative_word(text):\n",
    "# #     numNegWords=0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in negativeWordList:\n",
    "# #             numNegWords -=1\n",
    "# #     sumNeg = numNegWords \n",
    "# #     sumNeg = sumNeg * -1\n",
    "# #     return sumNeg\n",
    "# # #===============================================================================\n",
    "# # # Loading litigous words\n",
    "# # # litigous = '/content/litiguous.txt'\n",
    "# # # with open(litigous ,'r') as litfile:\n",
    "# # #     litigousword=litfile.read().lower()\n",
    "# # litigouswordlist =litigiuous_txt\n",
    "\n",
    "\n",
    "# # # Calculating litigous score \n",
    "# # def litigous_score(text):\n",
    "# #     numLitWords = 0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in litigouswordlist:\n",
    "# #             numLitWords  += 1\n",
    "    \n",
    "# #     sumLit = numLitWords\n",
    "# #     return sumLit\n",
    "# # #===============================================================================\t\n",
    "# # # Loading interesting words\n",
    "# # # interestingWordsfile = '/content/interestingwords.txt'\n",
    "# # # with open(interestingWordsfile ,'r') as intfile:\n",
    "# # #     interestingWord=intfile.read().lower()\n",
    "# # interestingWordList=interesting_txt\n",
    "\n",
    "\n",
    "# # # Calculating interestingword score \n",
    "# # def interesting_score(text):\n",
    "# #     numintwords = 0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in interestingWordList:\n",
    "# #             numintwords  += 1\n",
    "    \n",
    "# #     sumInt = numintwords\n",
    "# #     return sumInt\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # Loading superflous words\n",
    "# # # superflousfile = '/content/superfluous.txt'\n",
    "# # # with open(superflousfile ,'r') as supfile:\n",
    "# # #     supword=supfile.read().lower()\n",
    "# # supWordList=superfluous_txt\n",
    "\n",
    "\n",
    "# # # Calculating sup_score \n",
    "# # def sup_score(text):\n",
    "# #     numSupWords = 0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in supWordList:\n",
    "# #             numSupWords  += 1\n",
    "\n",
    "         \n",
    "# #     sumSup = numSupWords\n",
    "# #     return sumSup\n",
    "# # #===============================================================================\n",
    "# # # Loading Modalstrong words\n",
    "# # # modalstrongfile = '/content/modal_strong.txt'\n",
    "# # # with open(modalstrongfile ,'r') as modfile:\n",
    "# # #     modstrongword=modfile.read().lower()\n",
    "# # modstrongwordlist=modal_txt\n",
    "\n",
    "\n",
    "# # # Calculating modal strongscore \n",
    "# # def modal_strong_store(text):\n",
    "# #     nomodstr = 0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in modstrongwordlist:\n",
    "# #             nomodstr  += 1\n",
    "    \n",
    "# #     summodstr = nomodstr\n",
    "# #     return summodstr\n",
    "# # #===============================================================================\t\n",
    "# # # # Loading Modalneutral words\n",
    "# # # modalneutralfile = '/content/modal_neutral.txt'\n",
    "# # # with open(modalneutralfile ,'r') as modn:\n",
    "# # #     modneuword=modn.read().lower()\n",
    "# # # modneuwordlist=modneuword.split('\\n')\n",
    "\n",
    "\n",
    "# # # # Calculating modal_neu_score\n",
    "# # # def modal_neutral_score(text):\n",
    "# # #     nomodneu = 0\n",
    "# # #     rawToken = tokenizer(text)\n",
    "# # #     for word in rawToken:\n",
    "# # #         if word in modneuwordlist:\n",
    "# # #             nomodneu  += 1\n",
    "    \n",
    "# # #     summodneu = nomodneu\n",
    "# # #     return summodneu\n",
    "\n",
    "# # # #===============================================================================\n",
    "# # # # Loading modal_weak words\n",
    "# # # modal_weak_file = '/content/modal_weak.txt'\n",
    "# # # with open(modal_weak_file ,'r') as modweak:\n",
    "# # #     modweakword=modweak.read().lower()\n",
    "# # # modweaklist=modweakword.split('\\n')\n",
    "\n",
    "\n",
    "# # # # Calculating mod_weak_score \n",
    "# # # def mod_weak_score(text):\n",
    "# # #     nomodweak = 0\n",
    "# # #     rawToken = tokenizer(text)\n",
    "# # #     for word in rawToken:\n",
    "# # #         if word in modweaklist:\n",
    "# # #             nomodweak  += 1\n",
    "    \n",
    "# # #     summodweak = nomodweak\n",
    "# # #     return summodweak\n",
    "\t\n",
    "# # #===============================================================================\n",
    "# # # # Loading dictionary_countcount words\n",
    "# # # # dictionary_wordfile = '/content/dictionary.txt'\n",
    "# # # # with open(dictionary_wordfile ,'r') as dicfile:\n",
    "# # # #     dicword=dicfile.read().lower()\n",
    "# # # dicwordlist=\n",
    "\n",
    "\n",
    "# # # # Calculating dic_word_score score \n",
    "# # # def dic_word_score(text):\n",
    "# # #     nodicwords = 0\n",
    "# # #     rawToken = tokenizer(text)\n",
    "# # #     for word in rawToken:\n",
    "# # #         if word in dicwordlist:\n",
    "# # #             nodicwords  += 1\n",
    "    \n",
    "# # #     sumdic = nodickwords\n",
    "# # #     return sumdic\n",
    "# # #===============================================================================\n",
    "\n",
    "# # # Loading harvard_1 words\n",
    "# # # harvard_1_file = '/content/harvard_1.txt'\n",
    "# # # with open(harvard_1_file ,'r') as harfile:\n",
    "# # #     har_1_word=harfile.read().lower()\n",
    "# # har_1_list=Harvard_IV\n",
    "\n",
    "\n",
    "# # # Calculating dic_word_score score \n",
    "# # def har_1_score(text):\n",
    "# #     nohar_1_words = 0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in har_1_list:\n",
    "# #             nohar_1_words  += 1\n",
    "    \n",
    "# #     sumhar_1 = nohar_1_words\n",
    "# #     return sumhar_1\n",
    "# # #===============================================================================\n",
    "# # # Calculating polarity score\n",
    "# # def polarity_score(positiveScore, negativeScore):\n",
    "# #     pol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)\n",
    "# #     return pol_score\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # Calculating Average sentence length \n",
    "\n",
    "# # '''# It will calculated using formula --- \n",
    "# #       Average Sentence Length = the number of words / the number of sentences'''\n",
    "     \n",
    "# # def average_sentence_length(text):\n",
    "# #     sentence_list = sent_tokenize(text)\n",
    "# #     tokens = tokenizer(text)\n",
    "# #     totalWordCount = len(tokens)\n",
    "# #     totalSentences = len(sentence_list)\n",
    "# #     average_sent = 0\n",
    "# #     if totalSentences != 0:\n",
    "# #         average_sent = totalWordCount / totalSentences\n",
    "    \n",
    "# #     average_sent_length= average_sent\n",
    "    \n",
    "# #     return round(average_sent_length)\n",
    "# # #===============================================================================\n",
    "# # # Calculating percentage of complex words\n",
    "\n",
    "# # ''' It is calculated using Percentage of \n",
    "# #             Complex words = the number of complex words / the number of words''' \n",
    "\n",
    "# # def percentage_complex_word(text):\n",
    "# #     tokens = tokenizer(text)\n",
    "# #     complexWord = 0\n",
    "# #     complex_word_percentage = 0\n",
    "    \n",
    "# #     for word in tokens:\n",
    "# #         vowels=0\n",
    "# #         if word.endswith(('es','ed')):\n",
    "# #             pass\n",
    "# #         else:\n",
    "# #             for w in word:\n",
    "# #                 if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "# #                     vowels += 1\n",
    "# #             if(vowels > 2):\n",
    "# #                 complexWord += 1\n",
    "# #     if len(tokens) != 0:\n",
    "# #         complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "# #     return complex_word_percentage\n",
    "# # #===============================================================================\n",
    "\n",
    "# # # calculating Fog Index \n",
    "\n",
    "# # '''# Fog index is calculated using --\n",
    "# #     Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)'''\n",
    "\n",
    "# # def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "# #     fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "# #     return fogIndex\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # Counting complex words\n",
    "# # def complex_word_count(text):\n",
    "# #     tokens = tokenizer(text)\n",
    "# #     complexWord = 0\n",
    "    \n",
    "# #     for word in tokens:\n",
    "# #         vowels=0\n",
    "# #         if word.endswith(('es','ed')):\n",
    "# #             pass\n",
    "# #         else:\n",
    "# #             for w in word:\n",
    "# #                 if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "# #                     vowels += 1\n",
    "# #             if(vowels > 2):\n",
    "# #                 complexWord += 1\n",
    "# #     return complexWord\n",
    "# # #===============================================================================\n",
    "# # #Counting total words\n",
    "\n",
    "# # def total_word_count(text):\n",
    "# #     tokens = tokenizer(text)\n",
    "# #     return len(tokens)\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # calculating uncertainty_score\n",
    "# # # uncertainty_dictionaryFile= '/content/uncertainity.txt'\n",
    "# # # with open(uncertainty_dictionaryFile ,'r') as uncertain_dict:\n",
    "# # #     uncertainDict=uncertain_dict.read().lower()\n",
    "# # uncertainDictionary = uncertainty_txt\n",
    "\n",
    "# # #uncertainity score \n",
    "# # def uncertainty_score(text):\n",
    "# #     uncertainWordnum =0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in uncertainDictionary:\n",
    "# #             uncertainWordnum +=1\n",
    "# #     sumUncertainityScore = uncertainWordnum \n",
    "    \n",
    "# #     return sumUncertainityScore\n",
    "# # #===============================================================================\n",
    "# # # calculating constraining score\n",
    "# # # constraining_dictionaryFile = '/content/constrained.txt'\n",
    "# # # with open(constraining_dictionaryFile ,'r') as constraining_dict:\n",
    "# # #     constrainDict=constraining_dict.read().lower()\n",
    "# # constrainDictionary = constaining_txt\n",
    "\n",
    "\n",
    "# # def constraining_score(text):\n",
    "# #     constrainWordnum =0\n",
    "# #     rawToken = tokenizer(text)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in constrainDictionary:\n",
    "# #             constrainWordnum +=1\n",
    "# #     sumConstrainScore = constrainWordnum \n",
    "    \n",
    "# #     return sumConstrainScore\n",
    "# # #===============================================================================\n",
    "# # # Calculating positive word proportion\n",
    "# # def positive_word_prop(positiveScore,wordcount):\n",
    "# #     positive_word_proportion = 0\n",
    "# #     if wordcount !=0:\n",
    "# #         positive_word_proportion = positiveScore / wordcount\n",
    "        \n",
    "# #     return positive_word_proportion\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # Calculating negative word proportion\n",
    "\n",
    "# # def negative_word_prop(negativeScore,wordcount):\n",
    "# #     negative_word_proportion = 0\n",
    "# #     if wordcount !=0:\n",
    "# #         negative_word_proportion = negativeScore / wordcount\n",
    "        \n",
    "# #     return negative_word_proportion\n",
    "# # #===============================================================================\n",
    "# # # Calculating uncertain word proportion\n",
    "# # def uncertain_word_prop(uncertainScore,wordcount):\n",
    "# #     uncertain_word_proportion = 0\n",
    "# #     if wordcount !=0:\n",
    "# #         uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "# #     return uncertain_word_proportion\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # Calculating uncertain word proportion\n",
    "# # def uncertain_word_prop(uncertainScore,wordcount):\n",
    "# #     uncertain_word_proportion = 0\n",
    "# #     if wordcount !=0:\n",
    "# #         uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "# #     return uncertain_word_proportion\n",
    "\n",
    "# # #===============================================================================\n",
    "# # # Calculating constraining word proportion\n",
    "# # def constraining_word_prop(constrainingScore,wordcount):\n",
    "# #     constraining_word_proportion = 0\n",
    "# #     if wordcount !=0:\n",
    "# #         constraining_word_proportion = constrainingScore / wordcount\n",
    "        \n",
    "# #     return constraining_word_proportion\n",
    "# # #===============================================================================\n",
    "# # # calculating Constraining words for whole report\n",
    "# # def constrain_word_whole(mdaText):\n",
    "# #     wholeDoc = mdaText\n",
    "# #     constrainWordnumWhole =0\n",
    "# #     rawToken = tokenizer(wholeDoc)\n",
    "# #     for word in rawToken:\n",
    "# #         if word in constrainDictionary:\n",
    "# #             constrainWordnumWhole +=1\n",
    "# #     sumConstrainScoreWhole = constrainWordnumWhole \n",
    "    \n",
    "# #     return sumConstrainScoreWhole\n",
    "# # #===============================================================================\n",
    "  \n",
    "\n",
    "# #using contractions dictionary to make corrections \n",
    "# latest_ten_k = expand_contractions(re.sub('', \"'\", latest_ten_k))\n",
    "\n",
    "# #stripping the words using space \n",
    "# latest_ten_k = latest_ten_k.strip().lower()\n",
    "\n",
    "# #removing accented characters \n",
    "# latest_ten_k = remove_accented_chars(latest_ten_k)\n",
    "\n",
    "# #re-placing \" \" \" with space \n",
    "# latest_ten_k = latest_ten_k.replace('\"', '')\n",
    "\n",
    "# # Removing url's from the text\n",
    "# url_reg  = r'[a-z]*[:.]+\\S+'\n",
    "# latest_ten_k = re.sub(url_reg, '', latest_ten_k)\n",
    "\n",
    "# latest_ten_k = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", latest_ten_k)\n",
    "\n",
    "# #removing scrub_words\n",
    "# latest_ten_k = scrub_words(latest_ten_k)\n",
    "\n",
    "# #replace spaaces more than one with single space \n",
    "# latest_ten_k = re.sub(\"\\s+\", ' ', latest_ten_k)\n",
    "\n",
    "# #finding the length of the words in the data\n",
    "# word_count = len(latest_ten_k.split(' '))\n",
    "# #list of stop words from the observations by reading some level-1 cleaned documents \n",
    "\n",
    "# stopwords = ['dtd','copyright','llc','en','html','fe','ed','webfilings','e','vk','g','zip code', 'pagebreak','html' \n",
    "#              'w','c','en','table','body','par','value','per','securities','exchange','comission','telephone','number',\n",
    "#              'zip', 'code', 'end', 'page','xbrl','begin','dc','aa','aaa', 'aaa aa','ab','abn','abn amro','abnormal',\n",
    "#              'abo','abs','ac','az','ba','baa','aoci','aol','apb','api','app','ann','anne','amp','amt','anda','bla','bit',\n",
    "#              'bio','bhc','bb','bbb','bbl','bbls','bc','bcf','bcfe','apr','arc','aro','asa','asa','asc','asic','asp','asr',\n",
    "#              'asu','asus','ave','bms','bnp','bny','boe','blvd','bms','boe','bps','bs','btu', 'btus','ca','cad','cal','ccc',\n",
    "#              'cceeff','cdo','cdos','cds','ce','cede','cg','chk','cmsa','col','com','con','conway','ct','dd','de','dan',\n",
    "#              'dana','dea','wti','wto','wv','wyeth','wyoming','xannual','xerox','xi','xii','xiii','xindicate','xiv','xix','xl',\n",
    "#              'xthe','xv','xvi','xvii','xviii', 'xx','xxi','xxx','wi','vt','vs','von''vie','via','vi','var','ta','tab','tam',\n",
    "#              'td','tdr','tdrs','te','sur','ss','sr','sq','sp','sop','sip','sd','sdn','se',\n",
    "#              '__________________________________________ '\n",
    "#             '__________ ']\n",
    "# def review_to_words(raw_review):\n",
    "    \n",
    "#     remove = re.sub(r'\\b\\w{1,3}\\b', '', raw_review) #removing all words less than 4 characters \n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", remove) \n",
    "#     word = letters_only.lower().split()\n",
    "  \n",
    "#     meaningful_words = [w for w in word if not w in stopwords] \n",
    "#     return( \" \".join(meaningful_words))\n",
    " \n",
    "# data = review_to_words(latest_ten_k)\n",
    " \n",
    "# for k in sent_list:\n",
    "#     for i in range(len(k)):\n",
    "#         k[i] = k[i].lower()\n",
    "# #Let's define functions for creating new columns \n",
    "\n",
    "# # Generic stop wordss list from the mcdonald weblink\\\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# stopWordsFile = '/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/StopWords_Generic.txt'\n",
    "# with open(stopWordsFile ,'r') as stop_words:\n",
    "#     stopWords = stop_words.read().lower()\n",
    "# stopWordList = stopWords.split('\\n')\n",
    "# stopWordList[-1:] = []\n",
    "# # Tokenizer\n",
    "# def tokenizer(text):\n",
    "#     text = text.lower()\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
    "#     return filtered_words\n",
    "\n",
    "# #===============================================================================\n",
    "# # Loading positive words\n",
    "# positiveWordList=positive_txt\n",
    "\n",
    "# # Calculating positive score \n",
    "# def positive_score(text):\n",
    "#     numPosWords = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in positiveWordList:\n",
    "#             numPosWords  += 1\n",
    "    \n",
    "#     sumPos = numPosWords\n",
    "#     return sumPos\n",
    "# #===============================================================================\n",
    "# # Loading negative words\n",
    "\n",
    "# negativeWordList=negative_txt\n",
    "\n",
    "\n",
    "# # Calculating Negative score\n",
    "# def negative_word(text):\n",
    "#     numNegWords=0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in negativeWordList:\n",
    "#             numNegWords -=1\n",
    "#     sumNeg = numNegWords \n",
    "#     sumNeg = sumNeg * -1\n",
    "#     return sumNeg\n",
    "# #===============================================================================\n",
    "# # Loading litigous words\n",
    "\n",
    "# litigouswordlist =litigiuous_txt\n",
    "\n",
    "\n",
    "# # Calculating litigous score \n",
    "# def litigous_score(text):\n",
    "#     numLitWords = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in litigouswordlist:\n",
    "#             numLitWords  += 1\n",
    "    \n",
    "#     sumLit = numLitWords\n",
    "#     return sumLit\n",
    "# #===============================================================================\t\n",
    "# # Loading interesting words\n",
    "# interestingWordList=interesting_txt\n",
    "\n",
    "\n",
    "# # Calculating interestingword score \n",
    "# def interesting_score(text):\n",
    "#     numintwords = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in interestingWordList:\n",
    "#             numintwords  += 1\n",
    "    \n",
    "#     sumInt = numintwords\n",
    "#     return sumInt\n",
    "\n",
    "# #===============================================================================\n",
    "# # Loading superflous words\n",
    "# supWordList=superfluous_txt\n",
    "\n",
    "\n",
    "# # Calculating sup_score \n",
    "# def sup_score(text):\n",
    "#     numSupWords = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in supWordList:\n",
    "#             numSupWords  += 1\n",
    "\n",
    "         \n",
    "#     sumSup = numSupWords\n",
    "#     return sumSup\n",
    "# #===============================================================================\n",
    "# # Loading Modalstrong words\n",
    "# modstrongwordlist=modal_txt\n",
    "\n",
    "\n",
    "# # Calculating modal strongscore \n",
    "# def modal_strong_store(text):\n",
    "#     nomodstr = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in modstrongwordlist:\n",
    "#             nomodstr  += 1\n",
    "    \n",
    "#     summodstr = nomodstr\n",
    "#     return summodstr\n",
    "# #===============================================================================\t\n",
    "# har_1_list=Harvard_IV\n",
    "\n",
    "\n",
    "# # Calculating dic_word_score score \n",
    "# def har_1_score(text):\n",
    "#     nohar_1_words = 0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in har_1_list:\n",
    "#             nohar_1_words  += 1\n",
    "    \n",
    "#     sumhar_1 = nohar_1_words\n",
    "#     return sumhar_1\n",
    "# #===============================================================================\n",
    "# # Calculating polarity score\n",
    "# def polarity_score(positiveScore, negativeScore):\n",
    "#     pol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)\n",
    "#     return pol_score\n",
    "\n",
    "# #===============================================================================\n",
    "# # Calculating Average sentence length \n",
    "\n",
    "# '''# It will calculated using formula --- \n",
    "#       Average Sentence Length = the number of words / the number of sentences'''\n",
    "     \n",
    "# def average_sentence_length(text):\n",
    "#     sentence_list = sent_tokenize(text)\n",
    "#     tokens = tokenizer(text)\n",
    "#     totalWordCount = len(tokens)\n",
    "#     totalSentences = len(sentence_list)\n",
    "#     average_sent = 0\n",
    "#     if totalSentences != 0:\n",
    "#         average_sent = totalWordCount / totalSentences\n",
    "    \n",
    "#     average_sent_length= average_sent\n",
    "    \n",
    "#     return round(average_sent_length)\n",
    "# #===============================================================================\n",
    "# # Calculating percentage of complex words\n",
    "\n",
    "# ''' It is calculated using Percentage of \n",
    "#             Complex words = the number of complex words / the number of words''' \n",
    "\n",
    "# def percentage_complex_word(text):\n",
    "#     tokens = tokenizer(text)\n",
    "#     complexWord = 0\n",
    "#     complex_word_percentage = 0\n",
    "    \n",
    "#     for word in tokens:\n",
    "#         vowels=0\n",
    "#         if word.endswith(('es','ed')):\n",
    "#             pass\n",
    "#         else:\n",
    "#             for w in word:\n",
    "#                 if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "#                     vowels += 1\n",
    "#             if(vowels > 2):\n",
    "#                 complexWord += 1\n",
    "#     if len(tokens) != 0:\n",
    "#         complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "#     return complex_word_percentage\n",
    "# #===============================================================================\n",
    "\n",
    "# # calculating Fog Index \n",
    "\n",
    "# '''# Fog index is calculated using --\n",
    "#     Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)'''\n",
    "\n",
    "# def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "#     fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "#     return fogIndex\n",
    "\n",
    "# #===============================================================================\n",
    "# # Counting complex words\n",
    "# def complex_word_count(text):\n",
    "#     tokens = tokenizer(text)\n",
    "#     complexWord = 0\n",
    "    \n",
    "#     for word in tokens:\n",
    "#         vowels=0\n",
    "#         if word.endswith(('es','ed')):\n",
    "#             pass\n",
    "#         else:\n",
    "#             for w in word:\n",
    "#                 if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "#                     vowels += 1\n",
    "#             if(vowels > 2):\n",
    "#                 complexWord += 1\n",
    "#     return complexWord\n",
    "# #===============================================================================\n",
    "# #Counting total words\n",
    "\n",
    "# def total_word_count(text):\n",
    "#     tokens = tokenizer(text)\n",
    "#     return len(tokens)\n",
    "\n",
    "# #===============================================================================\n",
    "# # calculating uncertainty_score\n",
    "# # uncertainty_dictionaryFile= '/content/uncertainity.txt'\n",
    "# # with open(uncertainty_dictionaryFile ,'r') as uncertain_dict:\n",
    "# #     uncertainDict=uncertain_dict.read().lower()\n",
    "# uncertainDictionary = uncertainty_txt\n",
    "\n",
    "# #uncertainity score \n",
    "# def uncertainty_score(text):\n",
    "#     uncertainWordnum =0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in uncertainDictionary:\n",
    "#             uncertainWordnum +=1\n",
    "#     sumUncertainityScore = uncertainWordnum \n",
    "    \n",
    "#     return sumUncertainityScore\n",
    "# #===============================================================================\n",
    "# # calculating constraining score\n",
    "# # constraining_dictionaryFile = '/content/constrained.txt'\n",
    "# # with open(constraining_dictionaryFile ,'r') as constraining_dict:\n",
    "# #     constrainDict=constraining_dict.read().lower()\n",
    "# constrainDictionary = constaining_txt\n",
    "\n",
    "\n",
    "# def constraining_score(text):\n",
    "#     constrainWordnum =0\n",
    "#     rawToken = tokenizer(text)\n",
    "#     for word in rawToken:\n",
    "#         if word in constrainDictionary:\n",
    "#             constrainWordnum +=1\n",
    "#     sumConstrainScore = constrainWordnum \n",
    "    \n",
    "#     return sumConstrainScore\n",
    "# #===============================================================================\n",
    "# # Calculating positive word proportion\n",
    "# def positive_word_prop(positiveScore,wordcount):\n",
    "#     positive_word_proportion = 0\n",
    "#     if wordcount !=0:\n",
    "#         positive_word_proportion = positiveScore / wordcount\n",
    "        \n",
    "#     return positive_word_proportion\n",
    "\n",
    "# #===============================================================================\n",
    "# # Calculating negative word proportion\n",
    "\n",
    "# def negative_word_prop(negativeScore,wordcount):\n",
    "#     negative_word_proportion = 0\n",
    "#     if wordcount !=0:\n",
    "#         negative_word_proportion = negativeScore / wordcount\n",
    "        \n",
    "#     return negative_word_proportion\n",
    "# #===============================================================================\n",
    "# # Calculating uncertain word proportion\n",
    "# def uncertain_word_prop(uncertainScore,wordcount):\n",
    "#     uncertain_word_proportion = 0\n",
    "#     if wordcount !=0:\n",
    "#         uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "#     return uncertain_word_proportion\n",
    "\n",
    "# #===============================================================================\n",
    "# # Calculating uncertain word proportion\n",
    "# def uncertain_word_prop(uncertainScore,wordcount):\n",
    "#     uncertain_word_proportion = 0\n",
    "#     if wordcount !=0:\n",
    "#         uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "#     return uncertain_word_proportion\n",
    "\n",
    "# #===============================================================================\n",
    "# # Calculating constraining word proportion\n",
    "# def constraining_word_prop(constrainingScore,wordcount):\n",
    "#     constraining_word_proportion = 0\n",
    "#     if wordcount !=0:\n",
    "#         constraining_word_proportion = constrainingScore / wordcount\n",
    "        \n",
    "#     return constraining_word_proportion\n",
    "# #===============================================================================\n",
    "# # calculating Constraining words for whole report\n",
    "# def constrain_word_whole(mdaText):\n",
    "#     wholeDoc = mdaText\n",
    "#     constrainWordnumWhole =0\n",
    "#     rawToken = tokenizer(wholeDoc)\n",
    "#     for word in rawToken:\n",
    "#         if word in constrainDictionary:\n",
    "#             constrainWordnumWhole +=1\n",
    "#     sumConstrainScoreWhole = constrainWordnumWhole \n",
    "    \n",
    "#     return sumConstrainScoreWhole\n",
    "# #===============================================================================\n",
    "  \n",
    "# # final_df = pd.DataFrame(columns = ['year', 'company', 'positive_score', 'negative_score', 'litigous_score', 'superfluous_score', 'interesting_score', 'modal_score', 'harvard_1', 'polarity_score', 'avg_sen_length', 'fog_index'])\n",
    "\n",
    "# positive_score = positive_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "# negative_score = negative_word(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "# litigous_score = litigous_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "# superfluous_score = sup_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "# interesting_socre = interesting_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "# modal_score = modal_strong_store(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "# harvard_1 = har_1_score(latest_ten_k)/len(latest_ten_k.split(' '))\n",
    "# polarity_score = polarity_score(positive_score, negative_score)\n",
    "# avg_sen_length = average_sentence_length(latest_ten_k)\n",
    "# fog_index = fog_index(avg_sen_length, percentage_complex_word(latest_ten_k))\n",
    "\n",
    "# new_row = pd.DataFrame({\n",
    "#     'year': [max_ten_k_year],\n",
    "#     'company': [comp_curr],\n",
    "#     'positive_score': [positive_score],\n",
    "#     'negative_score': [negative_score],\n",
    "#     'litigous_score': [litigous_score],\n",
    "#     'superfluous_score': [superfluous_score],\n",
    "#     'interesting_score': [interesting_socre],\n",
    "#     'modal_score': [modal_score],\n",
    "#     'harvard_1': [harvard_1],\n",
    "#     'polarity_score': [polarity_score],\n",
    "#     'avg_sen_length': [avg_sen_length],\n",
    "#     'fog_index': [fog_index]\n",
    "# })\n",
    "# final_df = pd.concat([final_df, new_row], ignore_index=False)\n",
    "# # df = final_df.append({'year': max_ten_k_year, 'company': comp_curr, 'positive_score': positive_score}, ignore_index=True)\n",
    "# final_df\n",
    "# # negative_word(latest_ten_k)/len(latest_ten_k)\n",
    "# final_df\n",
    "# str_year = str(max_ten_k_year) + '-12-31'\n",
    "# str_year\n",
    "# # len(latest_ten_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Let's get next year outlooks\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "tickers_dict = {'AMAZON COM INC': 'AMZN', 'APPLE INC': 'AAPL', 'AMERICAN EXPRESS CO':'AXP', 'MICROSOFT CORP': 'MSFT', 'Facebook Inc': 'META', 'NETFLIX INC': 'NFLX'}\n",
    "\n",
    "tickers = ['AMZN', 'AAPL', 'AXP', 'MSFT', 'META', 'NFLX']  # Replace with the stock symbol you want\n",
    "\n",
    "annual_returns = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "    data = yf.download(ticker, start=\"2000-01-01\", end=\"2024-01-01\", interval='1d')\n",
    "\n",
    "    # Resample the data to get the price at the end of each year\n",
    "    data_yearly = data['Adj Close'].resample('Y').last()\n",
    "\n",
    "\n",
    "    # Calculate annual returns: (Price in current year - Price in previous year) / Price in previous year\n",
    "    annual_returns[ticker] = data_yearly.pct_change()\n",
    "\n",
    "# Drop rows with NaN values (usually the first year will have NaN)\n",
    "# annual_returns = data[['Yearly Close', 'Annual Return']].dropna()\n",
    "annual_returns = annual_returns.dropna()\n",
    "annual_returns.index = annual_returns.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AXP</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>META</th>\n",
       "      <th>NFLX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-06-30</th>\n",
       "      <td>0.072519</td>\n",
       "      <td>0.010853</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.047962</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.079603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-31</th>\n",
       "      <td>0.021677</td>\n",
       "      <td>0.045822</td>\n",
       "      <td>-0.005220</td>\n",
       "      <td>-0.036613</td>\n",
       "      <td>-0.301929</td>\n",
       "      <td>-0.169952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-31</th>\n",
       "      <td>0.064166</td>\n",
       "      <td>0.093876</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>0.052737</td>\n",
       "      <td>-0.168125</td>\n",
       "      <td>0.050484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-09-30</th>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>-0.024700</td>\n",
       "      <td>-0.034393</td>\n",
       "      <td>0.199336</td>\n",
       "      <td>-0.088413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-31</th>\n",
       "      <td>-0.084264</td>\n",
       "      <td>-0.107600</td>\n",
       "      <td>-0.012198</td>\n",
       "      <td>-0.040994</td>\n",
       "      <td>-0.025392</td>\n",
       "      <td>0.455547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <td>0.032391</td>\n",
       "      <td>-0.042384</td>\n",
       "      <td>-0.064484</td>\n",
       "      <td>-0.022226</td>\n",
       "      <td>-0.071281</td>\n",
       "      <td>-0.012051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30</th>\n",
       "      <td>-0.078907</td>\n",
       "      <td>-0.088678</td>\n",
       "      <td>-0.055700</td>\n",
       "      <td>-0.036643</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>-0.129312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-31</th>\n",
       "      <td>0.046963</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.017159</td>\n",
       "      <td>0.070815</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.090281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-30</th>\n",
       "      <td>0.097678</td>\n",
       "      <td>0.113780</td>\n",
       "      <td>0.169417</td>\n",
       "      <td>0.122945</td>\n",
       "      <td>0.085903</td>\n",
       "      <td>0.151279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.013582</td>\n",
       "      <td>0.097031</td>\n",
       "      <td>-0.007574</td>\n",
       "      <td>0.081950</td>\n",
       "      <td>0.027238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AMZN      AAPL       AXP      MSFT      META      NFLX\n",
       "Date                                                                  \n",
       "2012-06-30  0.072519  0.010853  0.042629  0.047962  0.050676  0.079603\n",
       "2012-07-31  0.021677  0.045822 -0.005220 -0.036613 -0.301929 -0.169952\n",
       "2012-08-31  0.064166  0.093876  0.010224  0.052737 -0.168125  0.050484\n",
       "2012-09-30  0.024369  0.002796 -0.024700 -0.034393  0.199336 -0.088413\n",
       "2012-10-31 -0.084264 -0.107600 -0.012198 -0.040994 -0.025392  0.455547\n",
       "...              ...       ...       ...       ...       ...       ...\n",
       "2023-08-31  0.032391 -0.042384 -0.064484 -0.022226 -0.071281 -0.012051\n",
       "2023-09-30 -0.078907 -0.088678 -0.055700 -0.036643  0.014600 -0.129312\n",
       "2023-10-31  0.046963 -0.002570 -0.017159  0.070815  0.003531  0.090281\n",
       "2023-11-30  0.097678  0.113780  0.169417  0.122945  0.085903  0.151279\n",
       "2023-12-31  0.040044  0.013582  0.097031 -0.007574  0.081950  0.027238\n",
       "\n",
       "[139 rows x 6 columns]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get next year outlooks\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "tickers_dict = {'AMAZON COM INC': 'AMZN', 'APPLE INC': 'AAPL', 'AMERICAN EXPRESS CO':'AXP', 'MICROSOFT CORP': 'MSFT', 'Facebook Inc': 'META', 'NETFLIX INC': 'NFLX'}\n",
    "\n",
    "tickers = ['AMZN', 'AAPL', 'AXP', 'MSFT', 'META', 'NFLX']  # Replace with the stock symbol you want\n",
    "\n",
    "annual_returns = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "    data = yf.download(ticker, start=\"2000-01-01\", end=\"2024-01-01\", interval='1d')\n",
    "\n",
    "    # Resample the data to get the price at the end of each year\n",
    "    data_yearly = data['Adj Close'].resample('M').last()\n",
    "\n",
    "\n",
    "    # Calculate annual returns: (Price in current year - Price in previous year) / Price in previous year\n",
    "    annual_returns[ticker] = data_yearly.pct_change()\n",
    "\n",
    "# Drop rows with NaN values (usually the first year will have NaN)\n",
    "# annual_returns = data[['Yearly Close', 'Annual Return']].dropna()\n",
    "annual_returns = annual_returns.dropna()\n",
    "annual_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>company</th>\n",
       "      <th>litigous_score</th>\n",
       "      <th>superfluous_score</th>\n",
       "      <th>interesting_score</th>\n",
       "      <th>modal_score</th>\n",
       "      <th>harvard_1</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>avg_sen_length</th>\n",
       "      <th>fog_index</th>\n",
       "      <th>Future Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.017580</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.014124</td>\n",
       "      <td>0.047509</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>33202</td>\n",
       "      <td>13280.973893</td>\n",
       "      <td>0.036627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.048322</td>\n",
       "      <td>0.051530</td>\n",
       "      <td>32329</td>\n",
       "      <td>12931.773862</td>\n",
       "      <td>-0.242095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.013058</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.011426</td>\n",
       "      <td>0.053947</td>\n",
       "      <td>0.057735</td>\n",
       "      <td>46143</td>\n",
       "      <td>18457.367887</td>\n",
       "      <td>0.085822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.012558</td>\n",
       "      <td>0.057231</td>\n",
       "      <td>0.061721</td>\n",
       "      <td>37799</td>\n",
       "      <td>15119.770491</td>\n",
       "      <td>0.362164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.012938</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.012615</td>\n",
       "      <td>0.058108</td>\n",
       "      <td>0.064072</td>\n",
       "      <td>38732</td>\n",
       "      <td>15492.970464</td>\n",
       "      <td>-0.026202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019</td>\n",
       "      <td>AMERICAN EXPRESS CO</td>\n",
       "      <td>0.013071</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.013307</td>\n",
       "      <td>0.056927</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>36912</td>\n",
       "      <td>14764.973992</td>\n",
       "      <td>0.325174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.017205</td>\n",
       "      <td>0.042083</td>\n",
       "      <td>0.045054</td>\n",
       "      <td>21236</td>\n",
       "      <td>8494.584065</td>\n",
       "      <td>0.427630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.010144</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.017583</td>\n",
       "      <td>0.044043</td>\n",
       "      <td>0.044256</td>\n",
       "      <td>20623</td>\n",
       "      <td>8249.385967</td>\n",
       "      <td>0.341451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.045590</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>20292</td>\n",
       "      <td>8116.984073</td>\n",
       "      <td>0.099274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.012631</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.019142</td>\n",
       "      <td>0.046903</td>\n",
       "      <td>0.047337</td>\n",
       "      <td>22332</td>\n",
       "      <td>8932.981426</td>\n",
       "      <td>0.533768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>0.049623</td>\n",
       "      <td>0.046973</td>\n",
       "      <td>22006</td>\n",
       "      <td>8802.581550</td>\n",
       "      <td>-0.257112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019</td>\n",
       "      <td>Facebook Inc</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.019629</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.041190</td>\n",
       "      <td>22623</td>\n",
       "      <td>9049.381815</td>\n",
       "      <td>0.565718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2014</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.015840</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.053188</td>\n",
       "      <td>15527</td>\n",
       "      <td>6210.976029</td>\n",
       "      <td>-0.072141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.014999</td>\n",
       "      <td>0.049593</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>14881</td>\n",
       "      <td>5952.577085</td>\n",
       "      <td>1.343784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.051843</td>\n",
       "      <td>0.059144</td>\n",
       "      <td>15498</td>\n",
       "      <td>6199.376255</td>\n",
       "      <td>0.082357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.052055</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>14747</td>\n",
       "      <td>5898.977419</td>\n",
       "      <td>0.550565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.008501</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.014792</td>\n",
       "      <td>0.053753</td>\n",
       "      <td>0.059575</td>\n",
       "      <td>15823</td>\n",
       "      <td>6329.379561</td>\n",
       "      <td>0.394353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019</td>\n",
       "      <td>NETFLIX INC</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>0.053841</td>\n",
       "      <td>0.053331</td>\n",
       "      <td>15553</td>\n",
       "      <td>6221.380493</td>\n",
       "      <td>0.208884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>0.056858</td>\n",
       "      <td>0.052366</td>\n",
       "      <td>17804</td>\n",
       "      <td>7121.778589</td>\n",
       "      <td>-0.221771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.012721</td>\n",
       "      <td>0.058703</td>\n",
       "      <td>0.051306</td>\n",
       "      <td>19099</td>\n",
       "      <td>7639.780617</td>\n",
       "      <td>1.177831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>0.056813</td>\n",
       "      <td>0.058526</td>\n",
       "      <td>18850</td>\n",
       "      <td>7540.181178</td>\n",
       "      <td>0.109456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>0.052101</td>\n",
       "      <td>18871</td>\n",
       "      <td>7548.580234</td>\n",
       "      <td>0.559564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.012871</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.053431</td>\n",
       "      <td>0.053791</td>\n",
       "      <td>19665</td>\n",
       "      <td>7866.181073</td>\n",
       "      <td>0.284317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>0.012524</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.013132</td>\n",
       "      <td>0.054081</td>\n",
       "      <td>0.052896</td>\n",
       "      <td>18299</td>\n",
       "      <td>7319.781168</td>\n",
       "      <td>0.230277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year              company  litigous_score  superfluous_score  \\\n",
       "0   2014  AMERICAN EXPRESS CO        0.017580           0.000224   \n",
       "1   2015  AMERICAN EXPRESS CO        0.018550           0.000172   \n",
       "2   2016  AMERICAN EXPRESS CO        0.013058           0.000177   \n",
       "3   2017  AMERICAN EXPRESS CO        0.013551           0.000149   \n",
       "4   2018  AMERICAN EXPRESS CO        0.012938           0.000081   \n",
       "5   2019  AMERICAN EXPRESS CO        0.013071           0.000118   \n",
       "6   2014         Facebook Inc        0.010787           0.000109   \n",
       "7   2015         Facebook Inc        0.010144           0.000141   \n",
       "8   2016         Facebook Inc        0.010719           0.000114   \n",
       "9   2017         Facebook Inc        0.012631           0.000104   \n",
       "10  2018         Facebook Inc        0.011533           0.000132   \n",
       "11  2019         Facebook Inc        0.012075           0.000360   \n",
       "12  2014          NETFLIX INC        0.008947           0.000194   \n",
       "13  2015          NETFLIX INC        0.009717           0.000242   \n",
       "14  2016          NETFLIX INC        0.009877           0.000312   \n",
       "15  2017          NETFLIX INC        0.007876           0.000328   \n",
       "16  2018          NETFLIX INC        0.008501           0.000343   \n",
       "17  2019          NETFLIX INC        0.009433           0.000272   \n",
       "18  2014       AMAZON COM INC        0.014040           0.000035   \n",
       "19  2015       AMAZON COM INC        0.014278           0.000032   \n",
       "20  2016       AMAZON COM INC        0.012388           0.000033   \n",
       "21  2017       AMAZON COM INC        0.013163           0.000033   \n",
       "22  2018       AMAZON COM INC        0.012871           0.000031   \n",
       "23  2019       AMAZON COM INC        0.012524           0.000034   \n",
       "\n",
       "    interesting_score  modal_score  harvard_1  polarity_score avg_sen_length  \\\n",
       "0            0.001607     0.014124   0.047509        0.053458          33202   \n",
       "1            0.001603     0.014447   0.048322        0.051530          32329   \n",
       "2            0.001374     0.011426   0.053947        0.057735          46143   \n",
       "3            0.001588     0.012558   0.057231        0.061721          37799   \n",
       "4            0.001629     0.012615   0.058108        0.064072          38732   \n",
       "5            0.001368     0.013307   0.056927        0.065217          36912   \n",
       "6            0.001775     0.017205   0.042083        0.045054          21236   \n",
       "7            0.001803     0.017583   0.044043        0.044256          20623   \n",
       "8            0.001686     0.018550   0.045590        0.046100          20292   \n",
       "9            0.001641     0.019142   0.046903        0.047337          22332   \n",
       "10           0.001561     0.020368   0.049623        0.046973          22006   \n",
       "11           0.001593     0.019629   0.050100        0.041190          22623   \n",
       "12           0.002517     0.015840   0.048567        0.053188          15527   \n",
       "13           0.002903     0.014999   0.049593        0.056433          14881   \n",
       "14           0.003006     0.014327   0.051843        0.059144          15498   \n",
       "15           0.002995     0.015301   0.052055        0.061659          14747   \n",
       "16           0.001830     0.014792   0.053753        0.059575          15823   \n",
       "17           0.001747     0.014984   0.053841        0.053331          15553   \n",
       "18           0.001498     0.012124   0.056858        0.052366          17804   \n",
       "19           0.001623     0.012721   0.058703        0.051306          19099   \n",
       "20           0.001610     0.013242   0.056813        0.058526          18850   \n",
       "21           0.001313     0.013196   0.055968        0.052101          18871   \n",
       "22           0.001632     0.013091   0.053431        0.053791          19665   \n",
       "23           0.001654     0.013132   0.054081        0.052896          18299   \n",
       "\n",
       "       fog_index  Future Return  \n",
       "0   13280.973893       0.036627  \n",
       "1   12931.773862      -0.242095  \n",
       "2   18457.367887       0.085822  \n",
       "3   15119.770491       0.362164  \n",
       "4   15492.970464      -0.026202  \n",
       "5   14764.973992       0.325174  \n",
       "6    8494.584065       0.427630  \n",
       "7    8249.385967       0.341451  \n",
       "8    8116.984073       0.099274  \n",
       "9    8932.981426       0.533768  \n",
       "10   8802.581550      -0.257112  \n",
       "11   9049.381815       0.565718  \n",
       "12   6210.976029      -0.072141  \n",
       "13   5952.577085       1.343784  \n",
       "14   6199.376255       0.082357  \n",
       "15   5898.977419       0.550565  \n",
       "16   6329.379561       0.394353  \n",
       "17   6221.380493       0.208884  \n",
       "18   7121.778589      -0.221771  \n",
       "19   7639.780617       1.177831  \n",
       "20   7540.181178       0.109456  \n",
       "21   7548.580234       0.559564  \n",
       "22   7866.181073       0.284317  \n",
       "23   7319.781168       0.230277  "
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_returns\n",
    "for i in range(len(final_df)):\n",
    "    final_df.loc[i, 'Future Return'] = annual_returns.loc[final_df.loc[i,'year']][tickers_dict[final_df.loc[i, 'company']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.to_csv('/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('/Users/rishikumra/Downloads/Projects/sentiment-analysis-sec-master/data/data_2cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
